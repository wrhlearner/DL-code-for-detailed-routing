{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f52802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1a007",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feec4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(strategy, output, defaultStrategy, defaultOutput) -> list:\n",
    "    \"\"\"Wrap up strategies and outputs to generate an input list\"\"\"\n",
    "    dataset = []\n",
    "    defaultInputSet = []\n",
    "    inputset = []\n",
    "#     coef = {'size' : None, 'offset' : None, 'mazeEndIter' : None, 'MarkerCost' : None, \n",
    "#              'FixedShapeCost' : None, 'Decay' : None, 'ripupMode': None, 'followGuide' : None}\n",
    "    # parse default strategy and output\n",
    "    # check input file exists\n",
    "    if not os.path.exists(defaultStrategy):\n",
    "        print(\"Default strategy {} doesn't exist!\".format(defaultStrategy))\n",
    "        return []\n",
    "    # parse default strategy\n",
    "    with open(defaultStrategy, 'r') as f:\n",
    "        line = f.readline()\n",
    "        temp = line.split(\"@\")\n",
    "        for i in range(len(temp) - 1):\n",
    "            element = temp[i]\n",
    "            coef = dict([])\n",
    "            # parse 1 strategy\n",
    "            result = element.split(\",\")[:-1]\n",
    "            coef['size'] = int(result[0])\n",
    "            coef['offset'] = int(result[1])\n",
    "            coef['mazeEndIter'] = int(result[2])\n",
    "            coef['MarkerCost'] = int(result[3])\n",
    "            coef['FixedShapeCost'] = int(result[4])\n",
    "            coef['Decay'] = float(result[5])\n",
    "            if result[6] == 'ALL':\n",
    "                coef['ripupMode'] = 0\n",
    "            elif result[6] == 'DRC':\n",
    "                coef['ripupMode'] = 1\n",
    "            else:\n",
    "                coef['ripupMode'] = 2\n",
    "            if result[7] == 'True':\n",
    "                coef['followGuide'] = 1\n",
    "            elif result[7] == 'False':\n",
    "                coef['followGuide'] = 0\n",
    "            # add result to list\n",
    "            defaultInputSet.append(coef)\n",
    "    # check input file exists\n",
    "    if not os.path.exists(defaultOutput):\n",
    "        print(\"Default strategy output {} doesn't exist!\".format(defaultOutput))\n",
    "        return []\n",
    "    # parse default strategy outputs\n",
    "    with open(defaultOutput, 'r') as f:\n",
    "        allLines = f.readlines()\n",
    "        for line in allLines:\n",
    "            result = line.split(',')[:-1]\n",
    "            datapoint = []\n",
    "            for i, value in enumerate(result):\n",
    "                datapoint.append([defaultInputSet[i], int(value)])\n",
    "            # add datapoint into dataset\n",
    "            dataset.append(datapoint)\n",
    "    # parse strategy and output\n",
    "    # check input file exists\n",
    "    if not os.path.exists(strategy):\n",
    "        print(\"Strategy {} doesn't exist!\".format(strategy))\n",
    "        return []\n",
    "    # parse strategy\n",
    "    with open(strategy, 'r') as f:\n",
    "        allLine = f.readlines()\n",
    "        for line in allLine:\n",
    "            temp = line.split(\"@\")\n",
    "            instrategy = []\n",
    "            for i in range(len(temp) - 1):\n",
    "                element = temp[i]\n",
    "                coef = dict([])\n",
    "                # parse 1 strategy\n",
    "                result = element.split(\",\")[:-1]\n",
    "                coef['size'] = int(result[0])\n",
    "                coef['offset'] = int(result[1])\n",
    "                coef['mazeEndIter'] = int(result[2])\n",
    "                coef['MarkerCost'] = int(result[3])\n",
    "                coef['FixedShapeCost'] = int(result[4])\n",
    "                coef['Decay'] = float(result[5])\n",
    "                if result[6] == 'ALL':\n",
    "                    coef['ripupMode'] = 0\n",
    "                elif result[6] == 'DRC':\n",
    "                    coef['ripupMode'] = 1\n",
    "                else:\n",
    "                    coef['ripupMode'] = 2\n",
    "                if result[7] == 'True':\n",
    "                    coef['followGuide'] = 1\n",
    "                elif result[7] == 'False':\n",
    "                    coef['followGuide'] = 0\n",
    "                # add result to list\n",
    "                instrategy.append(coef)\n",
    "            inputset.append(instrategy)\n",
    "    # check input file exists\n",
    "    if not os.path.exists(output):\n",
    "        print(\"Output {} doesn't exist!\".format(output))\n",
    "        return []\n",
    "    # parse strategy outputs\n",
    "    with open(output, 'r') as f:\n",
    "        allLines = f.readlines()\n",
    "        for index, line in enumerate(allLines):\n",
    "            result = line.split(',')[:-1]\n",
    "            datapoint = []\n",
    "            for i, value in enumerate(result):\n",
    "                if i > 63:\n",
    "                    break\n",
    "#                 print(\"design {} strategy {}\".format(index + 1, i + 1))\n",
    "                datapoint.append([inputset[index][i], int(value)])\n",
    "            # add datapoint into dataset\n",
    "            dataset.append(datapoint)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1583b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultStrategy = \"./DefaultStrategy.txt\"\n",
    "defaultOutput = \"./defaultOutputs.txt\"\n",
    "strategy = \"./strategy.txt\"\n",
    "output = \"./outputs.txt\"\n",
    "dataset = getData(strategy, output, defaultStrategy, defaultOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56b9c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd96033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'size': 7,\n",
       "   'offset': 0,\n",
       "   'mazeEndIter': 3,\n",
       "   'MarkerCost': 0,\n",
       "   'FixedShapeCost': 8,\n",
       "   'Decay': 0.95,\n",
       "   'ripupMode': 0,\n",
       "   'followGuide': 1},\n",
       "  10178],\n",
       " [{'size': 7,\n",
       "   'offset': -2,\n",
       "   'mazeEndIter': 3,\n",
       "   'MarkerCost': 8,\n",
       "   'FixedShapeCost': 8,\n",
       "   'Decay': 0.95,\n",
       "   'ripupMode': 0,\n",
       "   'followGuide': 1},\n",
       "  569],\n",
       " [{'size': 7,\n",
       "   'offset': -5,\n",
       "   'mazeEndIter': 3,\n",
       "   'MarkerCost': 8,\n",
       "   'FixedShapeCost': 8,\n",
       "   'Decay': 0.95,\n",
       "   'ripupMode': 0,\n",
       "   'followGuide': 1},\n",
       "  499],\n",
       " [{'size': 7,\n",
       "   'offset': 0,\n",
       "   'mazeEndIter': 8,\n",
       "   'MarkerCost': 32,\n",
       "   'FixedShapeCost': 16,\n",
       "   'Decay': 0.95,\n",
       "   'ripupMode': 1,\n",
       "   'followGuide': 0},\n",
       "  1],\n",
       " [{'size': 7,\n",
       "   'offset': -1,\n",
       "   'mazeEndIter': 8,\n",
       "   'MarkerCost': 32,\n",
       "   'FixedShapeCost': 16,\n",
       "   'Decay': 0.95,\n",
       "   'ripupMode': 1,\n",
       "   'followGuide': 0},\n",
       "  0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dab5c4",
   "metadata": {},
   "source": [
    "# Turning datapoint to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51338d2",
   "metadata": {},
   "source": [
    "To represent a single data point input, we use a tensor of size <No_Strategy x 1 x 8>. (8 is the number of items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612582f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a99924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_output = max([len(x) for x in dataset])\n",
    "n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a1af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetToTensor(dataset, dim):\n",
    "    \"\"\"Turning the input to tensor. Remain output format\"\"\"\n",
    "    Dataset = []\n",
    "    Label = []\n",
    "    for element in dataset:\n",
    "        datain = []\n",
    "        dataout = torch.zeros(dim)\n",
    "        for i, it in enumerate(element):\n",
    "            coef, drc = it\n",
    "            datain.append(list(coef.values()))\n",
    "            dataout[i] = drc\n",
    "        Dataset.append(torch.Tensor(datain).reshape(-1, 1, 8))\n",
    "        Label.append(torch.Tensor(dataout).reshape(1, dim))\n",
    "    \n",
    "    return Dataset, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7aed81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset, Label = datasetToTensor(dataset, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059c7d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf27658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6ec6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0000,  0.0000,  3.0000,  0.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000, -2.0000,  3.0000,  8.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000, -5.0000,  3.0000,  8.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000,  0.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -1.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc018c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0178e+04, 5.6900e+02, 4.9900e+02, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708a89b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "077d6b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5502a5",
   "metadata": {},
   "source": [
    "# train-test split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3e50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e31a5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(X, y, trainSize = 0.7):\n",
    "    \"\"\"split dataset into training set and test set with a set ratio. Randomly generate.\"\"\"\n",
    "    # generate a list of random integer number as indices to the datasets\n",
    "    trainIndex = random.sample(list(range(len(X))), int(trainSize*len(X)))\n",
    "    testIndex = [x for x in list(range(len(X))) if x not in trainIndex]\n",
    "    # generate datasets\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for i in trainIndex:\n",
    "        X_train.append(X[i])\n",
    "        Y_train.append(y[i])\n",
    "    for i in testIndex:\n",
    "        X_test.append(X[i])\n",
    "        Y_test.append(y[i])\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c3260c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = splitDataset(Dataset, Label, trainSize = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f4f40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abbffc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93522ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0000,  0.0000,  3.0000,  0.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000, -2.0000,  3.0000,  8.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000, -5.0000,  3.0000,  8.0000,  8.0000,  0.9500,  0.0000,\n",
       "           1.0000]],\n",
       "\n",
       "        [[ 7.0000,  0.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -1.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -2.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -3.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -4.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -5.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -6.0000,  8.0000, 32.0000, 16.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000,  0.0000,  8.0000, 32.0000, 24.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -1.0000,  8.0000, 32.0000, 24.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -2.0000,  8.0000, 32.0000, 24.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -3.0000,  8.0000, 32.0000, 24.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -4.0000,  8.0000, 32.0000, 24.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -5.0000,  8.0000, 32.0000, 32.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -6.0000,  8.0000, 32.0000, 32.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -3.0000,  8.0000, 32.0000, 32.0000,  0.9500,  0.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000,  0.0000,  8.0000, 32.0000, 32.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -1.0000,  8.0000, 32.0000, 32.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -2.0000,  8.0000, 32.0000, 80.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -3.0000,  8.0000, 32.0000, 80.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -4.0000,  8.0000, 32.0000, 80.0000,  0.9500,  1.0000,\n",
       "           0.0000]],\n",
       "\n",
       "        [[ 7.0000, -5.0000,  8.0000, 32.0000, 80.0000,  0.9500,  2.0000,\n",
       "           0.0000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afa50774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1f7885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5481e+04, 9.5940e+03, 7.1540e+03, 4.3700e+02, 1.3700e+02, 1.2100e+02,\n",
       "         9.9000e+01, 8.1000e+01, 7.7000e+01, 7.4000e+01, 3.4000e+01, 2.6000e+01,\n",
       "         2.6000e+01, 2.6000e+01, 2.6000e+01, 2.6000e+01, 2.6000e+01, 5.7000e+01,\n",
       "         5.7000e+01, 5.7000e+01, 5.7000e+01, 5.7000e+01, 5.7000e+01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f360a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a58e89",
   "metadata": {},
   "source": [
    "# Creating the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "640fcf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.outlayer = nn.Linear(output_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputData, hidden):\n",
    "        combined = torch.cat((inputData, hidden), dim=1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.h2o(self.relu(hidden))\n",
    "        # output = self.outlayer(self.softmax(output))\n",
    "        output = self.outlayer(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b099f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = len(X_train[0][0][0])\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21795248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bd9c514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4537b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_input, n_hidden, n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfdfff",
   "metadata": {},
   "source": [
    "# Run a step of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002fc02",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the Tensor for the current strategy) and a previous hidden state (which we initialize as zeros at first). We’ll get back the output (probability of drcs) and a next hidden state (which we keep for the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b00a2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47cd4ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputStrategy = X_train[0][0]\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(inputStrategy, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b60cce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5724,  0.4459, -1.2323, -0.1566, -0.1520, -1.0382, -0.8552, -0.5176,\n",
       "         -0.6286,  0.0109, -0.3621,  0.9232, -0.3782, -0.1108, -0.2838, -0.6337,\n",
       "         -1.0611, -0.1884, -0.5087,  0.3617,  0.0848, -0.0816, -0.8177,  0.2776,\n",
       "          0.4896,  1.0449, -0.9495, -0.1820, -0.0730, -0.6017, -0.0497,  0.8989,\n",
       "         -1.1807, -0.2463, -0.0074, -0.4291,  0.1424, -0.3874,  0.4756, -0.0845,\n",
       "          0.8303,  0.0571, -1.1675, -0.8472,  0.6416, -0.1940,  0.1324,  0.6732,\n",
       "         -0.6137,  0.4507,  0.3842,  0.8593, -0.4845, -0.1710, -0.5987, -0.8244,\n",
       "         -0.4766,  1.0357, -0.6464, -0.5399,  0.0911,  0.1352,  0.4861,  0.8142,\n",
       "         -0.0160,  0.9794,  0.4544, -0.2090, -0.9030,  0.8021, -0.2287, -0.9034,\n",
       "          1.1004,  0.1749,  0.6374, -0.9845,  0.6183, -0.7578, -0.5327,  0.2927,\n",
       "          0.0746, -0.2803, -0.5568, -0.4762,  0.6651, -0.6006,  0.3790,  0.4257,\n",
       "          0.1422,  0.3893,  0.4301,  0.2119, -0.8537,  0.6108, -0.8268, -0.2866,\n",
       "          0.6902,  1.0783,  0.4779, -0.6937, -0.3373, -0.3450,  0.2279, -0.6710,\n",
       "          0.5799,  0.5910, -0.0445,  0.2109,  0.4383,  0.9567,  0.0990, -0.0700,\n",
       "         -0.7501, -0.6210, -1.0637,  0.6562, -0.3309, -0.2413, -0.0491,  0.5627,\n",
       "          0.3033, -0.4030, -1.0056, -0.8965, -0.5744, -0.5718,  0.9377, -0.2460]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89b0c424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9170, -2.1424,  1.6979, -3.5462, -2.0619, -2.4780, -2.7946,  4.1279,\n",
       "         -1.6132, -1.8376,  2.7742, -2.0357, -1.5277, -3.6673,  2.1266, -3.6737,\n",
       "         -1.7656, -0.0081,  1.9126,  3.0296, -0.6114, -1.4707,  2.3480,  0.3219,\n",
       "          5.6781, -2.0574, -1.3228,  4.5766,  4.6140,  1.8436, -6.1460, -0.8085,\n",
       "          0.0462,  4.1142, -5.5046, -3.5784,  1.7815,  1.3945, -4.8730, -1.7396,\n",
       "         -0.9963,  5.0073, -0.3408, -0.2661, -1.0569, -0.2252,  0.4157, -2.1757,\n",
       "          0.1137,  2.6676, -1.6422, -3.5434,  0.4247, -0.1344, -0.6337,  1.1524,\n",
       "          0.9475, -0.8461,  2.7063,  0.4844,  1.4524,  1.6416, -4.5186, -1.9047,\n",
       "          3.1147]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb0ef9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1151d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3b49a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80d7e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c4b4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f6473b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 5000\n",
    "print_every = 10\n",
    "plot_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7bd1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1da393ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     rnn.train()\n",
    "#     with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "#         bar.set_description(f\"Epoch {epoch}\")\n",
    "#         for start in bar:\n",
    "#             # take a batch\n",
    "#             X_batch = X_train[start:start+batch_size]\n",
    "#             y_batch = Y_train[start:start+batch_size]\n",
    "#             # forward pass\n",
    "#             y_pred = rnn(X_batch)\n",
    "#             loss = loss_fn(y_pred, y_batch)\n",
    "#             # backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             # update weights\n",
    "#             optimizer.step()\n",
    "#             # print progress\n",
    "#             bar.set_postfix(mse=float(loss))\n",
    "#     # evaluate accuracy at end of each epoch\n",
    "#     rnn.eval()\n",
    "#     y_pred = rnn(X_test)\n",
    "#     mse = loss_fn(y_pred, Y_test)\n",
    "#     mse = float(mse)\n",
    "#     history.append(mse)\n",
    "#     if mse < best_mse:\n",
    "#         best_mse = mse\n",
    "#         best_weights = copy.deepcopy(model.state_dict())\n",
    "#  # restore model and return best accuracy\n",
    "# rnn.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f33214c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(y, x):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(x.size()[0]):\n",
    "        output, hidden = rnn(x[i], hidden)\n",
    "    \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    loss = criterion(output, softmax(y))\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30f7853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "429b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a08b602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6862d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8143fc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf6fdabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0% (0m 0s) nan\n",
      "20 0% (0m 0s) nan\n",
      "30 0% (0m 0s) nan\n",
      "40 0% (0m 0s) nan\n",
      "50 1% (0m 0s) nan\n",
      "60 1% (0m 0s) nan\n",
      "70 1% (0m 0s) nan\n",
      "80 1% (0m 0s) nan\n",
      "90 1% (0m 0s) nan\n",
      "100 2% (0m 0s) nan\n",
      "110 2% (0m 0s) nan\n",
      "120 2% (0m 0s) nan\n",
      "130 2% (0m 0s) nan\n",
      "140 2% (0m 0s) nan\n",
      "150 3% (0m 0s) nan\n",
      "160 3% (0m 0s) nan\n",
      "170 3% (0m 0s) nan\n",
      "180 3% (0m 1s) nan\n",
      "190 3% (0m 1s) nan\n",
      "200 4% (0m 1s) nan\n",
      "210 4% (0m 1s) nan\n",
      "220 4% (0m 1s) nan\n",
      "230 4% (0m 1s) nan\n",
      "240 4% (0m 1s) nan\n",
      "250 5% (0m 1s) nan\n",
      "260 5% (0m 1s) nan\n",
      "270 5% (0m 1s) nan\n",
      "280 5% (0m 1s) nan\n",
      "290 5% (0m 2s) nan\n",
      "300 6% (0m 2s) nan\n",
      "310 6% (0m 2s) nan\n",
      "320 6% (0m 2s) nan\n",
      "330 6% (0m 2s) nan\n",
      "340 6% (0m 2s) nan\n",
      "350 7% (0m 2s) nan\n",
      "360 7% (0m 2s) nan\n",
      "370 7% (0m 2s) nan\n",
      "380 7% (0m 2s) nan\n",
      "390 7% (0m 2s) nan\n",
      "400 8% (0m 2s) nan\n",
      "410 8% (0m 3s) nan\n",
      "420 8% (0m 3s) nan\n",
      "430 8% (0m 3s) nan\n",
      "440 8% (0m 3s) nan\n",
      "450 9% (0m 3s) nan\n",
      "460 9% (0m 3s) nan\n",
      "470 9% (0m 3s) nan\n",
      "480 9% (0m 3s) nan\n",
      "490 9% (0m 3s) nan\n",
      "500 10% (0m 3s) nan\n",
      "510 10% (0m 3s) nan\n",
      "520 10% (0m 3s) nan\n",
      "530 10% (0m 4s) nan\n",
      "540 10% (0m 4s) nan\n",
      "550 11% (0m 4s) nan\n",
      "560 11% (0m 4s) nan\n",
      "570 11% (0m 4s) nan\n",
      "580 11% (0m 4s) nan\n",
      "590 11% (0m 4s) nan\n",
      "600 12% (0m 4s) nan\n",
      "610 12% (0m 4s) nan\n",
      "620 12% (0m 4s) nan\n",
      "630 12% (0m 4s) nan\n",
      "640 12% (0m 5s) nan\n",
      "650 13% (0m 5s) nan\n",
      "660 13% (0m 5s) nan\n",
      "670 13% (0m 5s) nan\n",
      "680 13% (0m 5s) nan\n",
      "690 13% (0m 5s) nan\n",
      "700 14% (0m 5s) nan\n",
      "710 14% (0m 5s) nan\n",
      "720 14% (0m 5s) nan\n",
      "730 14% (0m 5s) nan\n",
      "740 14% (0m 5s) nan\n",
      "750 15% (0m 6s) nan\n",
      "760 15% (0m 6s) nan\n",
      "770 15% (0m 6s) nan\n",
      "780 15% (0m 6s) nan\n",
      "790 15% (0m 6s) nan\n",
      "800 16% (0m 6s) nan\n",
      "810 16% (0m 6s) nan\n",
      "820 16% (0m 6s) nan\n",
      "830 16% (0m 6s) nan\n",
      "840 16% (0m 6s) nan\n",
      "850 17% (0m 7s) nan\n",
      "860 17% (0m 7s) nan\n",
      "870 17% (0m 7s) nan\n",
      "880 17% (0m 7s) nan\n",
      "890 17% (0m 7s) nan\n",
      "900 18% (0m 7s) nan\n",
      "910 18% (0m 7s) nan\n",
      "920 18% (0m 7s) nan\n",
      "930 18% (0m 7s) nan\n",
      "940 18% (0m 7s) nan\n",
      "950 19% (0m 8s) nan\n",
      "960 19% (0m 8s) nan\n",
      "970 19% (0m 8s) nan\n",
      "980 19% (0m 8s) nan\n",
      "990 19% (0m 8s) nan\n",
      "1000 20% (0m 8s) nan\n",
      "1010 20% (0m 8s) nan\n",
      "1020 20% (0m 8s) nan\n",
      "1030 20% (0m 8s) nan\n",
      "1040 20% (0m 8s) nan\n",
      "1050 21% (0m 9s) nan\n",
      "1060 21% (0m 9s) nan\n",
      "1070 21% (0m 9s) nan\n",
      "1080 21% (0m 9s) nan\n",
      "1090 21% (0m 9s) nan\n",
      "1100 22% (0m 9s) nan\n",
      "1110 22% (0m 9s) nan\n",
      "1120 22% (0m 9s) nan\n",
      "1130 22% (0m 9s) nan\n",
      "1140 22% (0m 9s) nan\n",
      "1150 23% (0m 9s) nan\n",
      "1160 23% (0m 9s) nan\n",
      "1170 23% (0m 10s) nan\n",
      "1180 23% (0m 10s) nan\n",
      "1190 23% (0m 10s) nan\n",
      "1200 24% (0m 10s) nan\n",
      "1210 24% (0m 10s) nan\n",
      "1220 24% (0m 10s) nan\n",
      "1230 24% (0m 10s) nan\n",
      "1240 24% (0m 10s) nan\n",
      "1250 25% (0m 10s) nan\n",
      "1260 25% (0m 10s) nan\n",
      "1270 25% (0m 11s) nan\n",
      "1280 25% (0m 11s) nan\n",
      "1290 25% (0m 11s) nan\n",
      "1300 26% (0m 11s) nan\n",
      "1310 26% (0m 11s) nan\n",
      "1320 26% (0m 11s) nan\n",
      "1330 26% (0m 11s) nan\n",
      "1340 26% (0m 11s) nan\n",
      "1350 27% (0m 11s) nan\n",
      "1360 27% (0m 11s) nan\n",
      "1370 27% (0m 12s) nan\n",
      "1380 27% (0m 12s) nan\n",
      "1390 27% (0m 12s) nan\n",
      "1400 28% (0m 12s) nan\n",
      "1410 28% (0m 12s) nan\n",
      "1420 28% (0m 12s) nan\n",
      "1430 28% (0m 12s) nan\n",
      "1440 28% (0m 12s) nan\n",
      "1450 28% (0m 12s) nan\n",
      "1460 29% (0m 12s) nan\n",
      "1470 29% (0m 12s) nan\n",
      "1480 29% (0m 13s) nan\n",
      "1490 29% (0m 13s) nan\n",
      "1500 30% (0m 13s) nan\n",
      "1510 30% (0m 13s) nan\n",
      "1520 30% (0m 13s) nan\n",
      "1530 30% (0m 13s) nan\n",
      "1540 30% (0m 13s) nan\n",
      "1550 31% (0m 13s) nan\n",
      "1560 31% (0m 13s) nan\n",
      "1570 31% (0m 13s) nan\n",
      "1580 31% (0m 13s) nan\n",
      "1590 31% (0m 14s) nan\n",
      "1600 32% (0m 14s) nan\n",
      "1610 32% (0m 14s) nan\n",
      "1620 32% (0m 14s) nan\n",
      "1630 32% (0m 14s) nan\n",
      "1640 32% (0m 14s) nan\n",
      "1650 33% (0m 14s) nan\n",
      "1660 33% (0m 14s) nan\n",
      "1670 33% (0m 14s) nan\n",
      "1680 33% (0m 14s) nan\n",
      "1690 33% (0m 15s) nan\n",
      "1700 34% (0m 15s) nan\n",
      "1710 34% (0m 15s) nan\n",
      "1720 34% (0m 15s) nan\n",
      "1730 34% (0m 15s) nan\n",
      "1740 34% (0m 15s) nan\n",
      "1750 35% (0m 15s) nan\n",
      "1760 35% (0m 15s) nan\n",
      "1770 35% (0m 15s) nan\n",
      "1780 35% (0m 15s) nan\n",
      "1790 35% (0m 15s) nan\n",
      "1800 36% (0m 16s) nan\n",
      "1810 36% (0m 16s) nan\n",
      "1820 36% (0m 16s) nan\n",
      "1830 36% (0m 16s) nan\n",
      "1840 36% (0m 16s) nan\n",
      "1850 37% (0m 16s) nan\n",
      "1860 37% (0m 16s) nan\n",
      "1870 37% (0m 16s) nan\n",
      "1880 37% (0m 16s) nan\n",
      "1890 37% (0m 16s) nan\n",
      "1900 38% (0m 16s) nan\n",
      "1910 38% (0m 17s) nan\n",
      "1920 38% (0m 17s) nan\n",
      "1930 38% (0m 17s) nan\n",
      "1940 38% (0m 17s) nan\n",
      "1950 39% (0m 17s) nan\n",
      "1960 39% (0m 17s) nan\n",
      "1970 39% (0m 17s) nan\n",
      "1980 39% (0m 17s) nan\n",
      "1990 39% (0m 17s) nan\n",
      "2000 40% (0m 17s) nan\n",
      "2010 40% (0m 17s) nan\n",
      "2020 40% (0m 17s) nan\n",
      "2030 40% (0m 18s) nan\n",
      "2040 40% (0m 18s) nan\n",
      "2050 41% (0m 18s) nan\n",
      "2060 41% (0m 18s) nan\n",
      "2070 41% (0m 18s) nan\n",
      "2080 41% (0m 18s) nan\n",
      "2090 41% (0m 18s) nan\n",
      "2100 42% (0m 18s) nan\n",
      "2110 42% (0m 18s) nan\n",
      "2120 42% (0m 19s) nan\n",
      "2130 42% (0m 19s) nan\n",
      "2140 42% (0m 19s) nan\n",
      "2150 43% (0m 19s) nan\n",
      "2160 43% (0m 19s) nan\n",
      "2170 43% (0m 19s) nan\n",
      "2180 43% (0m 19s) nan\n",
      "2190 43% (0m 19s) nan\n",
      "2200 44% (0m 19s) nan\n",
      "2210 44% (0m 19s) nan\n",
      "2220 44% (0m 19s) nan\n",
      "2230 44% (0m 20s) nan\n",
      "2240 44% (0m 20s) nan\n",
      "2250 45% (0m 20s) nan\n",
      "2260 45% (0m 20s) nan\n",
      "2270 45% (0m 20s) nan\n",
      "2280 45% (0m 20s) nan\n",
      "2290 45% (0m 20s) nan\n",
      "2300 46% (0m 20s) nan\n",
      "2310 46% (0m 20s) nan\n",
      "2320 46% (0m 20s) nan\n",
      "2330 46% (0m 20s) nan\n",
      "2340 46% (0m 21s) nan\n",
      "2350 47% (0m 21s) nan\n",
      "2360 47% (0m 21s) nan\n",
      "2370 47% (0m 21s) nan\n",
      "2380 47% (0m 21s) nan\n",
      "2390 47% (0m 21s) nan\n",
      "2400 48% (0m 21s) nan\n",
      "2410 48% (0m 21s) nan\n",
      "2420 48% (0m 21s) nan\n",
      "2430 48% (0m 21s) nan\n",
      "2440 48% (0m 21s) nan\n",
      "2450 49% (0m 22s) nan\n",
      "2460 49% (0m 22s) nan\n",
      "2470 49% (0m 22s) nan\n",
      "2480 49% (0m 22s) nan\n",
      "2490 49% (0m 22s) nan\n",
      "2500 50% (0m 22s) nan\n",
      "2510 50% (0m 22s) nan\n",
      "2520 50% (0m 22s) nan\n",
      "2530 50% (0m 22s) nan\n",
      "2540 50% (0m 22s) nan\n",
      "2550 51% (0m 23s) nan\n",
      "2560 51% (0m 23s) nan\n",
      "2570 51% (0m 23s) nan\n",
      "2580 51% (0m 23s) nan\n",
      "2590 51% (0m 23s) nan\n",
      "2600 52% (0m 23s) nan\n",
      "2610 52% (0m 23s) nan\n",
      "2620 52% (0m 23s) nan\n",
      "2630 52% (0m 23s) nan\n",
      "2640 52% (0m 23s) nan\n",
      "2650 53% (0m 24s) nan\n",
      "2660 53% (0m 24s) nan\n",
      "2670 53% (0m 24s) nan\n",
      "2680 53% (0m 24s) nan\n",
      "2690 53% (0m 24s) nan\n",
      "2700 54% (0m 24s) nan\n",
      "2710 54% (0m 24s) nan\n",
      "2720 54% (0m 24s) nan\n",
      "2730 54% (0m 24s) nan\n",
      "2740 54% (0m 24s) nan\n",
      "2750 55% (0m 24s) nan\n",
      "2760 55% (0m 24s) nan\n",
      "2770 55% (0m 25s) nan\n",
      "2780 55% (0m 25s) nan\n",
      "2790 55% (0m 25s) nan\n",
      "2800 56% (0m 25s) nan\n",
      "2810 56% (0m 25s) nan\n",
      "2820 56% (0m 25s) nan\n",
      "2830 56% (0m 25s) nan\n",
      "2840 56% (0m 25s) nan\n",
      "2850 56% (0m 25s) nan\n",
      "2860 57% (0m 25s) nan\n",
      "2870 57% (0m 25s) nan\n",
      "2880 57% (0m 25s) nan\n",
      "2890 57% (0m 26s) nan\n",
      "2900 57% (0m 26s) nan\n",
      "2910 58% (0m 26s) nan\n",
      "2920 58% (0m 26s) nan\n",
      "2930 58% (0m 26s) nan\n",
      "2940 58% (0m 26s) nan\n",
      "2950 59% (0m 26s) nan\n",
      "2960 59% (0m 26s) nan\n",
      "2970 59% (0m 26s) nan\n",
      "2980 59% (0m 26s) nan\n",
      "2990 59% (0m 26s) nan\n",
      "3000 60% (0m 26s) nan\n",
      "3010 60% (0m 27s) nan\n",
      "3020 60% (0m 27s) nan\n",
      "3030 60% (0m 27s) nan\n",
      "3040 60% (0m 27s) nan\n",
      "3050 61% (0m 27s) nan\n",
      "3060 61% (0m 27s) nan\n",
      "3070 61% (0m 27s) nan\n",
      "3080 61% (0m 27s) nan\n",
      "3090 61% (0m 27s) nan\n",
      "3100 62% (0m 27s) nan\n",
      "3110 62% (0m 27s) nan\n",
      "3120 62% (0m 27s) nan\n",
      "3130 62% (0m 28s) nan\n",
      "3140 62% (0m 28s) nan\n",
      "3150 63% (0m 28s) nan\n",
      "3160 63% (0m 28s) nan\n",
      "3170 63% (0m 28s) nan\n",
      "3180 63% (0m 28s) nan\n",
      "3190 63% (0m 28s) nan\n",
      "3200 64% (0m 28s) nan\n",
      "3210 64% (0m 28s) nan\n",
      "3220 64% (0m 28s) nan\n",
      "3230 64% (0m 28s) nan\n",
      "3240 64% (0m 28s) nan\n",
      "3250 65% (0m 29s) nan\n",
      "3260 65% (0m 29s) nan\n",
      "3270 65% (0m 29s) nan\n",
      "3280 65% (0m 29s) nan\n",
      "3290 65% (0m 29s) nan\n",
      "3300 66% (0m 29s) nan\n",
      "3310 66% (0m 29s) nan\n",
      "3320 66% (0m 29s) nan\n",
      "3330 66% (0m 29s) nan\n",
      "3340 66% (0m 29s) nan\n",
      "3350 67% (0m 30s) nan\n",
      "3360 67% (0m 30s) nan\n",
      "3370 67% (0m 30s) nan\n",
      "3380 67% (0m 30s) nan\n",
      "3390 67% (0m 30s) nan\n",
      "3400 68% (0m 30s) nan\n",
      "3410 68% (0m 30s) nan\n",
      "3420 68% (0m 30s) nan\n",
      "3430 68% (0m 30s) nan\n",
      "3440 68% (0m 30s) nan\n",
      "3450 69% (0m 30s) nan\n",
      "3460 69% (0m 30s) nan\n",
      "3470 69% (0m 31s) nan\n",
      "3480 69% (0m 31s) nan\n",
      "3490 69% (0m 31s) nan\n",
      "3500 70% (0m 31s) nan\n",
      "3510 70% (0m 31s) nan\n",
      "3520 70% (0m 31s) nan\n",
      "3530 70% (0m 31s) nan\n",
      "3540 70% (0m 31s) nan\n",
      "3550 71% (0m 31s) nan\n",
      "3560 71% (0m 31s) nan\n",
      "3570 71% (0m 31s) nan\n",
      "3580 71% (0m 32s) nan\n",
      "3590 71% (0m 32s) nan\n",
      "3600 72% (0m 32s) nan\n",
      "3610 72% (0m 32s) nan\n",
      "3620 72% (0m 32s) nan\n",
      "3630 72% (0m 32s) nan\n",
      "3640 72% (0m 32s) nan\n",
      "3650 73% (0m 32s) nan\n",
      "3660 73% (0m 32s) nan\n",
      "3670 73% (0m 32s) nan\n",
      "3680 73% (0m 32s) nan\n",
      "3690 73% (0m 32s) nan\n",
      "3700 74% (0m 33s) nan\n",
      "3710 74% (0m 33s) nan\n",
      "3720 74% (0m 33s) nan\n",
      "3730 74% (0m 33s) nan\n",
      "3740 74% (0m 33s) nan\n",
      "3750 75% (0m 33s) nan\n",
      "3760 75% (0m 33s) nan\n",
      "3770 75% (0m 33s) nan\n",
      "3780 75% (0m 33s) nan\n",
      "3790 75% (0m 33s) nan\n",
      "3800 76% (0m 33s) nan\n",
      "3810 76% (0m 33s) nan\n",
      "3820 76% (0m 34s) nan\n",
      "3830 76% (0m 34s) nan\n",
      "3840 76% (0m 34s) nan\n",
      "3850 77% (0m 34s) nan\n",
      "3860 77% (0m 34s) nan\n",
      "3870 77% (0m 34s) nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3880 77% (0m 34s) nan\n",
      "3890 77% (0m 34s) nan\n",
      "3900 78% (0m 34s) nan\n",
      "3910 78% (0m 34s) nan\n",
      "3920 78% (0m 34s) nan\n",
      "3930 78% (0m 35s) nan\n",
      "3940 78% (0m 35s) nan\n",
      "3950 79% (0m 35s) nan\n",
      "3960 79% (0m 35s) nan\n",
      "3970 79% (0m 35s) nan\n",
      "3980 79% (0m 35s) nan\n",
      "3990 79% (0m 35s) nan\n",
      "4000 80% (0m 35s) nan\n",
      "4010 80% (0m 35s) nan\n",
      "4020 80% (0m 35s) nan\n",
      "4030 80% (0m 35s) nan\n",
      "4040 80% (0m 36s) nan\n",
      "4050 81% (0m 36s) nan\n",
      "4060 81% (0m 36s) nan\n",
      "4070 81% (0m 36s) nan\n",
      "4080 81% (0m 36s) nan\n",
      "4090 81% (0m 36s) nan\n",
      "4100 82% (0m 36s) nan\n",
      "4110 82% (0m 36s) nan\n",
      "4120 82% (0m 36s) nan\n",
      "4130 82% (0m 36s) nan\n",
      "4140 82% (0m 36s) nan\n",
      "4150 83% (0m 37s) nan\n",
      "4160 83% (0m 37s) nan\n",
      "4170 83% (0m 37s) nan\n",
      "4180 83% (0m 37s) nan\n",
      "4190 83% (0m 37s) nan\n",
      "4200 84% (0m 37s) nan\n",
      "4210 84% (0m 37s) nan\n",
      "4220 84% (0m 37s) nan\n",
      "4230 84% (0m 37s) nan\n",
      "4240 84% (0m 37s) nan\n",
      "4250 85% (0m 37s) nan\n",
      "4260 85% (0m 38s) nan\n",
      "4270 85% (0m 38s) nan\n",
      "4280 85% (0m 38s) nan\n",
      "4290 85% (0m 38s) nan\n",
      "4300 86% (0m 38s) nan\n",
      "4310 86% (0m 38s) nan\n",
      "4320 86% (0m 38s) nan\n",
      "4330 86% (0m 38s) nan\n",
      "4340 86% (0m 38s) nan\n",
      "4350 87% (0m 38s) nan\n",
      "4360 87% (0m 38s) nan\n",
      "4370 87% (0m 38s) nan\n",
      "4380 87% (0m 38s) nan\n",
      "4390 87% (0m 39s) nan\n",
      "4400 88% (0m 39s) nan\n",
      "4410 88% (0m 39s) nan\n",
      "4420 88% (0m 39s) nan\n",
      "4430 88% (0m 39s) nan\n",
      "4440 88% (0m 39s) nan\n",
      "4450 89% (0m 39s) nan\n",
      "4460 89% (0m 39s) nan\n",
      "4470 89% (0m 39s) nan\n",
      "4480 89% (0m 39s) nan\n",
      "4490 89% (0m 40s) nan\n",
      "4500 90% (0m 40s) nan\n",
      "4510 90% (0m 40s) nan\n",
      "4520 90% (0m 40s) nan\n",
      "4530 90% (0m 40s) nan\n",
      "4540 90% (0m 40s) nan\n",
      "4550 91% (0m 40s) nan\n",
      "4560 91% (0m 40s) nan\n",
      "4570 91% (0m 40s) nan\n",
      "4580 91% (0m 40s) nan\n",
      "4590 91% (0m 40s) nan\n",
      "4600 92% (0m 41s) nan\n",
      "4610 92% (0m 41s) nan\n",
      "4620 92% (0m 41s) nan\n",
      "4630 92% (0m 41s) nan\n",
      "4640 92% (0m 41s) nan\n",
      "4650 93% (0m 41s) nan\n",
      "4660 93% (0m 41s) nan\n",
      "4670 93% (0m 41s) nan\n",
      "4680 93% (0m 41s) nan\n",
      "4690 93% (0m 41s) nan\n",
      "4700 94% (0m 41s) nan\n",
      "4710 94% (0m 41s) nan\n",
      "4720 94% (0m 42s) nan\n",
      "4730 94% (0m 42s) nan\n",
      "4740 94% (0m 42s) nan\n",
      "4750 95% (0m 42s) nan\n",
      "4760 95% (0m 42s) nan\n",
      "4770 95% (0m 42s) nan\n",
      "4780 95% (0m 42s) nan\n",
      "4790 95% (0m 42s) nan\n",
      "4800 96% (0m 42s) nan\n",
      "4810 96% (0m 42s) nan\n",
      "4820 96% (0m 43s) nan\n",
      "4830 96% (0m 43s) nan\n",
      "4840 96% (0m 43s) nan\n",
      "4850 97% (0m 43s) nan\n",
      "4860 97% (0m 43s) nan\n",
      "4870 97% (0m 43s) nan\n",
      "4880 97% (0m 43s) nan\n",
      "4890 97% (0m 43s) nan\n",
      "4900 98% (0m 43s) nan\n",
      "4910 98% (0m 43s) nan\n",
      "4920 98% (0m 43s) nan\n",
      "4930 98% (0m 44s) nan\n",
      "4940 98% (0m 44s) nan\n",
      "4950 99% (0m 44s) nan\n",
      "4960 99% (0m 44s) nan\n",
      "4970 99% (0m 44s) nan\n",
      "4980 99% (0m 44s) nan\n",
      "4990 99% (0m 44s) nan\n",
      "5000 100% (0m 44s) nan\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1, n_iters + 1):\n",
    "    # randomly generate an index in training set\n",
    "    index = random.randint(0, len(X_train) - 1)\n",
    "    x = X_train[index]\n",
    "    y = Y_train[index]\n",
    "    output, loss = train(y, x)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        print('%d %d%% (%s) %.4f' % (iter, iter / n_iters * 100, timeSince(start), loss))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a791e8",
   "metadata": {},
   "source": [
    "# Result evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5393b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE: %.2f\" % best_mse)\n",
    "# print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "# plt.plot(history)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d6218730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff5ca568e50>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "45eef52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd70b",
   "metadata": {},
   "source": [
    "# Running one unseen inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95fc9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given an input\n",
    "def evaluate(x):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(x.size()[0]):\n",
    "        output, hidden = rnn(x[i], hidden)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f99ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a37a4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for index in range(len(X_test)):\n",
    "    x = X_test[index]\n",
    "    output = predict(x)\n",
    "    Y_predict.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61f44abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c31d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
