{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YsQPzmYoEupi"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM5DrM8lEupk"
   },
   "source": [
    "\n",
    "# Reinforcement Learning (DQN) Tutorial\n",
    "**Author**: [Adam Paszke](https://github.com/apaszke)\n",
    "            [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n",
    "\n",
    "\n",
    "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\n",
    "on the CartPole-v1 task from [Gymnasium](https://gymnasium.farama.org)_.\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find more\n",
    "information about the environment and other more challenging environments at\n",
    "[Gymnasium's website](https://gymnasium.farama.org/environments/classic_control/cart_pole/)_.\n",
    "\n",
    ".. figure:: /_static/img/cartpole.gif\n",
    "   :alt: CartPole\n",
    "\n",
    "   CartPole\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more than 2.4\n",
    "units away from center. This means better performing scenarios will run\n",
    "for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "We take these 4 inputs without any scaling and pass them through a\n",
    "small fully-connected network with 2 outputs, one for each action.\n",
    "The network is trained to predict the expected value for each action,\n",
    "given the input state. The action with the highest expected value is\n",
    "then chosen.\n",
    "\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "[gymnasium](https://gymnasium.farama.org/)_ for the environment,\n",
    "installed by using `pip`. This is a fork of the original OpenAI\n",
    "Gym project and maintained by the same team since Gym v0.19.\n",
    "If you are running this in Google Colab, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uUnFXf2REupk"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip3 install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dha1qo8BEupl"
   },
   "source": [
    "We'll also use the following from PyTorch:\n",
    "\n",
    "-  neural networks (``torch.nn``)\n",
    "-  optimization (``torch.optim``)\n",
    "-  automatic differentiation (``torch.autograd``)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GZUdaeq7Eupl"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import DREnv_fake\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DREnv(gym.Env):\n",
    "    # draw bar chart and line curve for visuliazation\n",
    "    # metadata = (\"render_modes\": [])\n",
    "\n",
    "    def __init__(self, actions, window_size=2, render_mode=None, drcmax=1e6, input_size=9):\n",
    "        self.size = drcmax\n",
    "        self.maxdrc = None\n",
    "        self.window_size = window_size\n",
    "        self.input_size = input_size\n",
    "        self._index = -1\n",
    "        if actions is None:\n",
    "            # index: (size, offset, mazeEndIter, DRCCost, MarkerCost, FixedShapeCost, Decay, ripupMode, followGuide)\n",
    "            # We have 65 actions, corresponding to default DRC sequences\n",
    "            # shapeCost = 8\n",
    "            # MARKERCOST = 32\n",
    "            # ripupMode: ALL->0, DRC->0.5, NEARDRC->1\n",
    "            # followGuide: True->1, False->0\n",
    "            actions = [[7,  0,  3,      8,       0,       8, 0.950, 0    ,  1],\n",
    "                        [7, -2,  3,      8,       8,       8, 0.950, 0    ,  1],\n",
    "                        [7, -5,  3,      8,       8,       8, 0.950, 0    ,  1],\n",
    "                        [7,  0,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7,  0,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,  2 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,  2 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,   4 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8,  4 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  4 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,      8,      32,  10 * 8, 0.950, 1, 0],\n",
    "                        [7, -6,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [5, -2,  8,      8,      32,  10 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,      8,      32,  50 * 8, 0.950, 1, 0],\n",
    "                        [7, -5,  8,  8 * 8,  2 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,  8 * 8,  2 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [3, -1,  8,      8,      32,  50 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,  50 * 8, 0.950, 1, 0],\n",
    "                        [7, -4,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [3, -2,  8,      8,      32, 100 * 8, 0.990, 0    , 0],\n",
    "                        [7,  0, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -1, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -2, 16,      8,      32, 100 * 8, 0.990, 1, 0],\n",
    "                        [7, -3, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -4, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -5, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -6, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [3, -0,  8,      8,      32, 100 * 8, 0.990, 0    , 0],\n",
    "                        [7,  0, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -1, 32,      8,      32, 100 * 8, 0.999, 1, 0],\n",
    "                        [7, -2, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -3, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -4, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -5, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -6, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [3, -1,  8,      8,      32, 100 * 8, 0.999, 0    , 0],\n",
    "                        [7,  0, 64,      8,      32, 100 * 8, 0.999, 1, 0],\n",
    "                        [7, -1, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -2, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -3, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -4, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -5, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -6, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0]]\n",
    "        columns = ['size', 'offset', 'mazeEndIter', 'DRCCost', 'MarkerCost', 'FixedShapeCost', 'Decay', 'ripupMode', 'followGuide']\n",
    "        self.actions = pd.DataFrame(actions, columns=columns)\n",
    "        self.action_space = spaces.Discrete(self.actions.shape[0])\n",
    "        # Observations are boxes containing the historical DRC sequence settings and DRC values\n",
    "        self.observations = None\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.window_size, self.input_size), dtype=float)\n",
    "\n",
    "    def _preprocessing(self):\n",
    "        if 'size' in self.actions.columns:\n",
    "            # action space preprocessing\n",
    "            # drop columns: size\n",
    "            self.actions = self.actions.drop(columns=['size'])\n",
    "    \n",
    "            # normalize coefficients\n",
    "            for column in ['offset', 'mazeEndIter', 'DRCCost', 'MarkerCost', 'FixedShapeCost', 'Decay']:\n",
    "                sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "                self.actions[column] = sc.fit_transform(self.actions[column].to_numpy().reshape(-1, 1))\n",
    "        \n",
    "        # observation space preprocessing\n",
    "        # padding historical data\n",
    "        self.observations = np.ones((self.window_size, self.input_size), dtype=float) * -1\n",
    "        # generate drc value before 0th iteration\n",
    "        self.maxdrc = self.np_random.integers(1, self.size, size=1, dtype=int)[0]\n",
    "        self.observations[-1, -1] = 1\n",
    "        # parameters initialization\n",
    "        self._index = -1\n",
    "        self._reward = -1   # reward starting value\n",
    "        self._curr_drc = 1\n",
    "\n",
    "    def _action_to_setting(self, action):\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the setting we will use for next iteration.\n",
    "        I.e. 0 corresponds to the 0th default setting\n",
    "        \"\"\"\n",
    "        # return selected value\n",
    "        return list(self.actions.iloc[action, :])\n",
    "\n",
    "    def _get_obs(self, action):\n",
    "        \"\"\"get DRC value for current iteration\"\"\"\n",
    "        if action:\n",
    "            # update DRC value\n",
    "            # randomly generate DRC value for current iteration\n",
    "            if int(self._curr_drc * self.maxdrc) == 0:\n",
    "                drc = 0\n",
    "            else:\n",
    "                drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n",
    "            # print(f\"self._curr_drc {self._curr_drc}, self.maxdrc {self.maxdrc}\")\n",
    "            self._curr_drc = drc\n",
    "            # generate observation for current iteration\n",
    "            setting = self._action_to_setting(action)\n",
    "            line = np.append(setting, drc)\n",
    "            # update the whole observation\n",
    "            self.observations = np.r_[self.observations, [line]]\n",
    "            self.observations = np.delete(self.observations, 0, 0)\n",
    "            \n",
    "        return self.observations\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"\"provide index for current iteration\"\"\"\n",
    "        return {\n",
    "            \"iteration index\": self._index\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._preprocessing()\n",
    "        observation = self._get_obs(None)\n",
    "        info = self._get_info()\n",
    "    \n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self._render_frame()\n",
    "    \n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the setting we use\n",
    "        reward = self._reward\n",
    "        \n",
    "        # update index\n",
    "        self._index += 1\n",
    "\n",
    "        # update observation\n",
    "        observation = self._get_obs(action)\n",
    "\n",
    "        # update truncated\n",
    "        if self._index == 64 and self._curr_drc != 0:\n",
    "            # punish truncated case\n",
    "            truncated = True\n",
    "            reward = -255    # -255 is a large value without detailed thought\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        # An episode is done iff current DRC value goes to zero\n",
    "        if self._curr_drc:\n",
    "            terminated = False\n",
    "        else:\n",
    "            terminated = True\n",
    "            # udpate reward: encourage finishing detailed routing using less iterations\n",
    "            reward = 0\n",
    "\n",
    "        # update info\n",
    "        info = self._get_info()\n",
    "    \n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self._render_frame()\n",
    "    \n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in notebook\n",
    "actions = None\n",
    "window_size = 2\n",
    "input_size = 9\n",
    "env = DREnv(actions, window_size=window_size, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>-4</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>7</td>\n",
       "      <td>-6</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    size  offset  mazeEndIter  DRCCost  MarkerCost  FixedShapeCost  Decay  \\\n",
       "0      7       0            3        8           0               8  0.950   \n",
       "1      7      -2            3        8           8               8  0.950   \n",
       "2      7      -5            3        8           8               8  0.950   \n",
       "3      7       0            8        8          32              16  0.950   \n",
       "4      7      -1            8        8          32              16  0.950   \n",
       "..   ...     ...          ...      ...         ...             ...    ...   \n",
       "60     7      -2           64      512         512             800  0.999   \n",
       "61     7      -3           64      512         512             800  0.999   \n",
       "62     7      -4           64      512         512             800  0.999   \n",
       "63     7      -5           64      512         512             800  0.999   \n",
       "64     7      -6           64      512         512             800  0.999   \n",
       "\n",
       "    ripupMode  followGuide  \n",
       "0         0.0            1  \n",
       "1         0.0            1  \n",
       "2         0.0            1  \n",
       "3         0.5            0  \n",
       "4         0.5            0  \n",
       "..        ...          ...  \n",
       "60        0.5            0  \n",
       "61        0.5            0  \n",
       "62        0.5            0  \n",
       "63        0.5            0  \n",
       "64        0.5            0  \n",
       "\n",
       "[65 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iteration index': -1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.96875</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.96875</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.87500</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.87500</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      offset  mazeEndIter  DRCCost  MarkerCost  FixedShapeCost  Decay  \\\n",
       "0   1.000000    -1.000000     -1.0    -1.00000       -1.000000   -1.0   \n",
       "1   0.333333    -1.000000     -1.0    -0.96875       -1.000000   -1.0   \n",
       "2  -0.666667    -1.000000     -1.0    -0.96875       -1.000000   -1.0   \n",
       "3   1.000000    -0.836066     -1.0    -0.87500       -0.979798   -1.0   \n",
       "4   0.666667    -0.836066     -1.0    -0.87500       -0.979798   -1.0   \n",
       "..       ...          ...      ...         ...             ...    ...   \n",
       "60  0.333333     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "61  0.000000     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "62 -0.333333     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "63 -0.666667     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "64 -1.000000     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "\n",
       "    ripupMode  followGuide  \n",
       "0         0.0            1  \n",
       "1         0.0            1  \n",
       "2         0.0            1  \n",
       "3         0.5            0  \n",
       "4         0.5            0  \n",
       "..        ...          ...  \n",
       "60        0.5            0  \n",
       "61        0.5            0  \n",
       "62        0.5            0  \n",
       "63        0.5            0  \n",
       "64        0.5            0  \n",
       "\n",
       "[65 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   offset          65 non-null     float64\n",
      " 1   mazeEndIter     65 non-null     float64\n",
      " 2   DRCCost         65 non-null     float64\n",
      " 3   MarkerCost      65 non-null     float64\n",
      " 4   FixedShapeCost  65 non-null     float64\n",
      " 5   Decay           65 non-null     float64\n",
      " 6   ripupMode       65 non-null     float64\n",
      " 7   followGuide     65 non-null     int64  \n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 4.2 KB\n"
     ]
    }
   ],
   "source": [
    "env.actions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.056410</td>\n",
       "      <td>-0.532913</td>\n",
       "      <td>-0.603907</td>\n",
       "      <td>-0.545192</td>\n",
       "      <td>-0.040559</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.046154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.665544</td>\n",
       "      <td>0.593202</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>0.565018</td>\n",
       "      <td>0.886749</td>\n",
       "      <td>0.915474</td>\n",
       "      <td>0.240942</td>\n",
       "      <td>0.211451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.939394</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.010101</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.573770</td>\n",
       "      <td>-0.523810</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offset  mazeEndIter    DRCCost  MarkerCost  FixedShapeCost  \\\n",
       "count  65.000000    65.000000  65.000000   65.000000       65.000000   \n",
       "mean    0.056410    -0.532913  -0.603907   -0.545192       -0.040559   \n",
       "std     0.665544     0.593202   0.595453    0.565018        0.886749   \n",
       "min    -1.000000    -1.000000  -1.000000   -1.000000       -1.000000   \n",
       "25%    -0.666667    -0.836066  -1.000000   -0.875000       -0.939394   \n",
       "50%     0.000000    -0.836066  -0.904762   -0.875000       -0.010101   \n",
       "75%     0.666667    -0.573770  -0.523810   -0.500000        1.000000   \n",
       "max     1.000000     1.000000   1.000000    1.000000        1.000000   \n",
       "\n",
       "           Decay  ripupMode  followGuide  \n",
       "count  65.000000  65.000000    65.000000  \n",
       "mean   -0.287284   0.476923     0.046154  \n",
       "std     0.915474   0.240942     0.211451  \n",
       "min    -1.000000   0.000000     0.000000  \n",
       "25%    -1.000000   0.500000     0.000000  \n",
       "50%    -1.000000   0.500000     0.000000  \n",
       "75%     0.632653   0.500000     0.000000  \n",
       "max     1.000000   1.000000     1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design 1 iteration 0 DRC: 0.8284404213607753, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-0.66666667 -0.83606557 -0.77777778 -0.75       -0.01010101 -1.\n",
      "   0.5         0.          0.82844042]]\n",
      "Design 1 iteration 1 DRC: 0.062370062370062374, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.77777778 -0.75       -0.01010101 -1.\n",
      "   0.5         0.          0.82844042]\n",
      " [-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.06237006]]\n",
      "Design 1 iteration 2 DRC: 0.012490366472667358, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.06237006]\n",
      " [-1.         -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01249037]]\n",
      "Design 1 iteration 3 DRC: 0.005355934559474382, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01249037]\n",
      " [ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00535593]]\n",
      "Design 1 iteration 4 DRC: 0.003323950226605094, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00535593]\n",
      " [-0.66666667 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00332395]]\n",
      "Design 1 iteration 5 DRC: 0.001463682879612083, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00332395]\n",
      " [ 0.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00146368]]\n",
      "Design 1 iteration 6 DRC: 0.0007931689347618551, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.46368288e-03]\n",
      " [-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   7.93168935e-04]]\n",
      "Design 1 iteration 7 DRC: 0.0006725581946820885, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   7.93168935e-04]\n",
      " [-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.72558195e-04]]\n",
      "Design 1 iteration 8 DRC: 0.0006091864498944145, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.72558195e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   6.09186450e-04]]\n",
      "Design 1 iteration 9 DRC: 0.000547858954938601, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   6.09186450e-04]\n",
      " [-3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.47858955e-04]]\n",
      "Design 1 iteration 10 DRC: 0.0004108942162039507, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.47858955e-04]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.10894216e-04]]\n",
      "Design 1 iteration 11 DRC: 0.000294371975787905, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.10894216e-04]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.94371976e-04]]\n",
      "Design 1 iteration 12 DRC: 0.00018602673469930107, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.94371976e-04]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.86026735e-04]]\n",
      "Design 1 iteration 13 DRC: 0.00017580548553999882, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.86026735e-04]\n",
      " [ 3.33333333e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.75805486e-04]]\n",
      "Design 1 iteration 14 DRC: 0.0001308319892390689, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.75805486e-04]\n",
      " [-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.30831989e-04]]\n",
      "Design 1 iteration 15 DRC: 2.0442498318604515e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.30831989e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.04424983e-06]]\n",
      "Design 1 iteration 16 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.04424983e-06]\n",
      " [-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 2 iteration 0 DRC: 0.897646875890844, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.89764688]]\n",
      "Design 2 iteration 1 DRC: 0.25044704174981214, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.89764688]\n",
      " [ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.25044704]]\n",
      "Design 2 iteration 2 DRC: 0.1027202598457166, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.25044704]\n",
      " [-0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.10272026]]\n",
      "Design 2 iteration 3 DRC: 0.08515173504029855, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.10272026]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.08515174]]\n",
      "Design 2 iteration 4 DRC: 0.0033884468862570294, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.08515174]\n",
      " [ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00338845]]\n",
      "Design 2 iteration 5 DRC: 0.0028614991231934762, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00338845]\n",
      " [ 0.33333333 -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.0028615 ]]\n",
      "Design 2 iteration 6 DRC: 0.002090514076416064, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.0028615 ]\n",
      " [-0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.00209051]]\n",
      "Design 2 iteration 7 DRC: 0.0010862898558236366, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.00209051]\n",
      " [ 0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.00108629]]\n",
      "Design 2 iteration 8 DRC: 0.0008854450117051512, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.08628986e-03]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   8.85445012e-04]]\n",
      "Design 2 iteration 9 DRC: 0.0004967130553467921, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   8.85445012e-04]\n",
      " [-3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   4.96713055e-04]]\n",
      "Design 2 iteration 10 DRC: 0.0002569950155924707, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   4.96713055e-04]\n",
      " [-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.56995016e-04]]\n",
      "Design 2 iteration 11 DRC: 0.00023107955183524677, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.56995016e-04]\n",
      " [-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.31079552e-04]]\n",
      "Design 2 iteration 12 DRC: 0.000127417696806351, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.31079552e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.27417697e-04]]\n",
      "Design 2 iteration 13 DRC: 1.9436597817917953e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.27417697e-04]\n",
      " [-6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.94365978e-05]]\n",
      "Design 2 iteration 14 DRC: 1.2957731878611968e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.94365978e-05]\n",
      " [-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.29577319e-05]]\n",
      "Design 2 iteration 15 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.29577319e-05]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 3 iteration 0 DRC: 0.09110677035344414, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.09110677]]\n",
      "Design 3 iteration 1 DRC: 0.07881546456344672, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.09110677]\n",
      " [ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.07881546]]\n",
      "Design 3 iteration 2 DRC: 0.040983304463199796, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.07881546]\n",
      " [ 1.         -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.0409833 ]]\n",
      "Design 3 iteration 3 DRC: 0.024398334131868944, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.0409833 ]\n",
      " [-1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.02439833]]\n",
      "Design 3 iteration 4 DRC: 0.005952161574466517, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.02439833]\n",
      " [ 1.         -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.00595216]]\n",
      "Design 3 iteration 5 DRC: 0.002119190653447831, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.00595216]\n",
      " [-0.66666667 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00211919]]\n",
      "Design 3 iteration 6 DRC: 0.0007371097925035935, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.11919065e-03]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   7.37109793e-04]]\n",
      "Design 3 iteration 7 DRC: 0.00033169940662661706, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   7.37109793e-04]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.31699407e-04]]\n",
      "Design 3 iteration 8 DRC: 0.00012899421368812886, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.31699407e-04]\n",
      " [-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.28994214e-04]]\n",
      "Design 3 iteration 9 DRC: 5.52832344377695e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.28994214e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.52832344e-05]]\n",
      "Design 3 iteration 10 DRC: 1.8427744812589837e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.52832344e-05]\n",
      " [ 6.66666667e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.84277448e-05]]\n",
      "Design 3 iteration 11 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[ 6.66666667e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.84277448e-05]\n",
      " [-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 4 iteration 0 DRC: 0.10261914658777282, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.10261915]]\n",
      "Design 4 iteration 1 DRC: 0.01899703388404907, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.10261915]\n",
      " [-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01899703]]\n",
      "Design 4 iteration 2 DRC: 0.005008644388419015, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01899703]\n",
      " [ 1.          1.         -1.         -0.875       1.          1.\n",
      "   1.          0.          0.00500864]]\n",
      "Design 4 iteration 3 DRC: 0.003470778928312895, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.          1.         -1.         -0.875       1.          1.\n",
      "   1.          0.          0.00500864]\n",
      " [-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00347078]]\n",
      "Design 4 iteration 4 DRC: 0.0026839388022882396, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00347078]\n",
      " [ 0.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00268394]]\n",
      "Design 4 iteration 5 DRC: 0.001545462537184979, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00268394]\n",
      " [-0.33333333 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00154546]]\n",
      "Design 4 iteration 6 DRC: 0.0009376963708762795, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.54546254e-03]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   9.37696371e-04]]\n",
      "Design 4 iteration 7 DRC: 0.0005024923839302284, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   9.37696371e-04]\n",
      " [-3.33333333e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.02492384e-04]]\n",
      "Design 4 iteration 8 DRC: 0.00010961496928067619, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.02492384e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.09614969e-04]]\n",
      "Design 4 iteration 9 DRC: 4.883835264980622e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.09614969e-04]\n",
      " [-6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.88383526e-05]]\n",
      "Design 4 iteration 10 DRC: 1.0852967255512493e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.88383526e-05]\n",
      " [-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.08529673e-05]]\n",
      "Design 4 iteration 11 DRC: 5.426483627756247e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.08529673e-05]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.42648363e-06]]\n",
      "Design 4 iteration 12 DRC: 2.1705934511024987e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.42648363e-06]\n",
      " [-3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.17059345e-06]]\n",
      "Design 4 iteration 13 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.17059345e-06]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 5 iteration 0 DRC: 0.9827461799660442, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.98274618]]\n",
      "Design 5 iteration 1 DRC: 0.896415770609319, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.98274618]\n",
      " [ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.89641577]]\n",
      "Design 5 iteration 2 DRC: 0.27161148839841537, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.89641577]\n",
      " [ 1.         -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.27161149]]\n",
      "Design 5 iteration 3 DRC: 0.06215336728919072, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.27161149]\n",
      " [ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.06215337]]\n",
      "Design 5 iteration 4 DRC: 0.06215336728919072, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.27161149]\n",
      " [ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.06215337]]\n",
      "Design 5 iteration 5 DRC: 0.011738351254480287, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.06215337]\n",
      " [ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.01173835]]\n",
      "Design 5 iteration 6 DRC: 0.005701754385964913, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.01173835]\n",
      " [ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00570175]]\n",
      "Design 5 iteration 7 DRC: 0.002280230145255612, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00570175]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00228023]]\n",
      "Design 5 iteration 8 DRC: 0.002280230145255612, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00570175]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00228023]]\n",
      "Design 5 iteration 9 DRC: 0.0007875872476891153, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.28023015e-03]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   7.87587248e-04]]\n",
      "Design 5 iteration 10 DRC: 0.0005564987738162611, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   7.87587248e-04]\n",
      " [-6.66666667e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.56498774e-04]]\n",
      "Design 5 iteration 11 DRC: 0.0003489907564610451, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.56498774e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.48990756e-04]]\n",
      "Design 5 iteration 12 DRC: 0.00032776834559517074, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.48990756e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.27768346e-04]]\n",
      "Design 5 iteration 13 DRC: 3.301263912469345e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.27768346e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  0.00000000e+00  0.00000000e+00\n",
      "   3.30126391e-05]]\n",
      "Design 5 iteration 14 DRC: 2.829654782116582e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  0.00000000e+00  0.00000000e+00\n",
      "   3.30126391e-05]\n",
      " [-1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.82965478e-05]]\n",
      "Design 5 iteration 15 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.82965478e-05]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 6 iteration 0 DRC: 0.48069389469397106, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.66666667 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.48069389]]\n",
      "Design 6 iteration 1 DRC: 0.1643978058062135, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.48069389]\n",
      " [ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.16439781]]\n",
      "Design 6 iteration 2 DRC: 0.154427205966578, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.16439781]\n",
      " [ 1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.15442721]]\n",
      "Design 6 iteration 3 DRC: 0.08273917857733769, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.15442721]\n",
      " [ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.08273918]]\n",
      "Design 6 iteration 4 DRC: 0.042264957808860776, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.08273918]\n",
      " [-1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.04226496]]\n",
      "Design 6 iteration 5 DRC: 0.0209135686194652, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.04226496]\n",
      " [ 1.         -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.02091357]]\n",
      "Design 6 iteration 6 DRC: 0.012551704827480878, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875       1.          0.63265306\n",
      "   0.          0.          0.02091357]\n",
      " [-0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.0125517 ]]\n",
      "Design 6 iteration 7 DRC: 0.010142418958648865, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.0125517 ]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.01014242]]\n",
      "Design 6 iteration 8 DRC: 0.009418233190362857, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.01014242]\n",
      " [ 0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00941823]]\n",
      "Design 6 iteration 9 DRC: 0.003228926703236563, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00941823]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00322893]]\n",
      "Design 6 iteration 10 DRC: 0.0005740031309261687, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.22892670e-03]\n",
      " [-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.74003131e-04]]\n",
      "Design 6 iteration 11 DRC: 0.00043654783571546755, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.74003131e-04]\n",
      " [ 1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.36547836e-04]]\n",
      "Design 6 iteration 12 DRC: 0.0002583650456275216, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.36547836e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.58365046e-04]]\n",
      "Design 6 iteration 13 DRC: 0.0002367285639739853, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.58365046e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.36728564e-04]]\n",
      "Design 6 iteration 14 DRC: 1.2727342149138996e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.36728564e-04]\n",
      " [-1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.27273421e-06]]\n",
      "Design 6 iteration 15 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.27273421e-06]\n",
      " [-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 7 iteration 0 DRC: 0.5019532048922158, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.33333333 -0.83606557 -1.         -0.875      -0.81818182 -1.\n",
      "   0.          0.          0.5019532 ]]\n",
      "Design 7 iteration 1 DRC: 0.06185319262077146, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -1.         -0.875      -0.81818182 -1.\n",
      "   0.          0.          0.5019532 ]\n",
      " [-1.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.06185319]]\n",
      "Design 7 iteration 2 DRC: 0.0018032205723947042, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.06185319]\n",
      " [ 1.         -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.00180322]]\n",
      "Design 7 iteration 3 DRC: 0.0014384859764660968, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.00180322]\n",
      " [-0.66666667 -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.00143849]]\n",
      "Design 7 iteration 4 DRC: 0.0012902060239156816, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.00143849]\n",
      " [ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.00129021]]\n",
      "Design 7 iteration 5 DRC: 0.001240779373065543, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.00129021]\n",
      " [-0.66666667 -0.83606557 -0.77777778 -0.75       -0.01010101 -1.\n",
      "   0.5         0.          0.00124078]]\n",
      "Design 7 iteration 6 DRC: 0.0007482172318348536, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.24077937e-03]\n",
      " [ 0.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   7.48217232e-04]]\n",
      "Design 7 iteration 7 DRC: 0.0004601791630874954, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   7.48217232e-04]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.60179163e-04]]\n",
      "Design 7 iteration 8 DRC: 0.000398821941342496, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.60179163e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   3.98821941e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12350/1370369620.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if int(self._curr_drc * self.maxdrc) == 0:\n",
      "/tmp/ipykernel_12350/1370369620.py:136: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "sample = 100\n",
    "num = 1\n",
    "env.reset()\n",
    "\n",
    "for _ in range(sample):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"Design {} iteration {} DRC: {}, reward: {}, terminated: {}, truncated: {}\\nobservation {}\".format(num, \n",
    "        info['iteration index'], observation[-1, -1], reward, terminated, truncated, observation))\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        num += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EEcLRIAEupl"
   },
   "source": [
    "## Replay Memory\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classes:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment. It essentially maps (state, action) pairs\n",
    "   to their (next_state, reward) result, with the state being the\n",
    "   screen difference image as described later on.\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o03vx5sMEupl"
   },
   "outputs": [],
   "source": [
    "# a data structure containing several named elements\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCS1wkgyEupm"
   },
   "source": [
    "Now, let's define our model. But first, let's quickly recap what a DQN is.\n",
    "\n",
    "## DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. A lower $\\gamma$ makes\n",
    "rewards from the uncertain far future less important for our agent\n",
    "than the ones in the near future that it can be fairly confident\n",
    "about. It also encourages agents to collect reward closer in time\n",
    "than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\\end{align}\n",
    "\n",
    "To minimize this error, we will use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss)_. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "### Q-network\n",
    "\n",
    "Our model will be a feed forward  neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/img/RLLSTM model structure.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1XHTvTBFEupm"
   },
   "outputs": [],
   "source": [
    "# Q-network model\n",
    "# the target is to learn the Q-function, which takes \n",
    "#     state/observation X action pairs as input,\n",
    "#     and make predictions about return (cumulation of rewards) \n",
    "#     Q:State x Action -> Return\n",
    "# inputs:\n",
    "#     n_observations: number of elements in observations, \n",
    "#         which is time series input for current state \n",
    "#         with historical data\n",
    "#     n_actions: number of actions in environment action space\n",
    "# outputs:\n",
    "#     model output with dimension n_actions: each represents the \n",
    "#         expected return of the state x action pair\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.layer1 = nn.Linear(n_observations, 128)\n",
    "#         self.layer2 = nn.Linear(128, 128)\n",
    "#         self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.layer1(x))\n",
    "#         x = F.relu(self.layer2(x))\n",
    "#         return self.layer3(x)\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self,input_size=9, seq_len=2, coeff_size=8, hidden_size=10,\n",
    "#                  output_size=65, num_layers=10,dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.seq_len = seq_len\n",
    "#         self.coeff_size = coeff_size\n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#         # initializing h0 and c0:\n",
    "#         self.hidden = (torch.zeros(num_layers, self.hidden_size).to(device),    # h0\n",
    "#                        torch.zeros(num_layers, self.hidden_size).to(device))    # c0\n",
    "\n",
    "#         # add an LSTM layer:\n",
    "#         self.lstm = nn.LSTM(input_size,hidden_size, \n",
    "#                             num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "#         # TODO: change fully-connected layer structure\n",
    "#         # add a fully-connected layer:\n",
    "#         self.in_features = seq_len * hidden_size + output_size * coeff_size\n",
    "#         self.linear = nn.Linear(self.in_features, self.output_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # TODO: parse inputs\n",
    "#         seq = inputs[:self.seq_len, :].view(inputs.shape[0] - self.output_size, -1).to(device)\n",
    "#         coeff = inputs[self.seq_len:, :self.coeff_size].reshape((inputs.shape[0] - self.seq_len) * self.coeff_size).to(device)\n",
    "        \n",
    "#         lstm_out, self.hidden = self.lstm(seq, self.hidden)\n",
    "        \n",
    "#         data = torch.cat((lstm_out.view(lstm_out.shape[0] * lstm_out.shape[1]), coeff), 0)\n",
    "        \n",
    "#         pred = self.linear(data)\n",
    "        \n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple CNN\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(int(input_size), 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, int(output_size))\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        \n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VkObo6VEupm"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Hyperparameters and utilities\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the duration of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gFR0vqAlEupm"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 1\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSTM-based DQN\n",
    "# # instantiate\n",
    "# seq_len = len(observation)\n",
    "# coeff_size = len(env.actions.columns) - 1\n",
    "# hidden_size = 10         # from LSTM notebook experience\n",
    "# output_size = n_actions\n",
    "# num_layers = 10          # from LSTM notebook experience\n",
    "# dropout = 0.2\n",
    "# policy_net = DQN(input_size=input_size, seq_len=seq_len, coeff_size=coeff_size, \n",
    "#                  hidden_size=hidden_size, output_size=output_size, \n",
    "#                  num_layers=num_layers,dropout=dropout).to(device)\n",
    "# target_net = DQN(input_size=input_size, seq_len=seq_len, coeff_size=coeff_size, \n",
    "#                  hidden_size=hidden_size, output_size=output_size, \n",
    "#                  num_layers=num_layers,dropout=dropout).to(device)\n",
    "\n",
    "# ANN-based DQN\n",
    "# instantiate\n",
    "seq_len = len(observation)\n",
    "output_size = n_actions\n",
    "\n",
    "policy_net = DQN(input_size=input_size * (seq_len + n_actions), output_size=output_size).to(device)\n",
    "target_net = DQN(input_size=input_size * (seq_len + n_actions), output_size=output_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "memory = ReplayMemory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "# create actions for network input\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    global actions\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        # select action according to policy\n",
    "        with torch.no_grad():\n",
    "            # t.min(0) will return the smallest drc value of the tensor returned by the policy network.\n",
    "            # we pick action with the smallest expected DRC value.\n",
    "            # create network input\n",
    "            # inputs shape torch.Size([67, 9])\n",
    "            inputs = np.vstack((state, actions))\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            # select the action with minimum DRC output as optimum action\n",
    "            return policy_net(inputs).min(0).indices.view(1, 1)\n",
    "    else:\n",
    "        # select action randomly from action space\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _  = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    select_action(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udnRhi1vEupm"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By definition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network is updated at every step with a\n",
    "[soft update](https://arxiv.org/pdf/1509.02971.pdf)_ controlled by\n",
    "the hyperparameter ``TAU``, which was previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1uDRpRM7Eupm"
   },
   "outputs": [],
   "source": [
    "# batch version\n",
    "# def optimize_model():\n",
    "#     if len(memory) < BATCH_SIZE:\n",
    "#         return\n",
    "#     transitions = memory.sample(BATCH_SIZE)\n",
    "#     # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "#     # detailed explanation). This converts batch-array of Transitions\n",
    "#     # to Transition of batch-arrays.\n",
    "#     batch = Transition(*zip(*transitions))\n",
    "\n",
    "#     # Compute a mask of non-final states and concatenate the batch elements\n",
    "#     # (a final state would've been the one after which simulation ended)\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                                 if s is not None])\n",
    "#     state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(batch.action)\n",
    "#     reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "#     # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "#     # columns of actions taken. These are the actions which would've been taken\n",
    "#     # for each batch state according to policy_net\n",
    "#     state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "#     # Compute V(s_{t+1}) for all next states.\n",
    "#     # Expected values of actions for non_final_next_states are computed based\n",
    "#     # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "#     # This is merged based on the mask, such that we'll have either the expected\n",
    "#     # state value or 0 in case the state was final.\n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "#     with torch.no_grad():\n",
    "#         # TODO: change max to min to select DRC setting that yields minimum DRC values\n",
    "#         # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "#         next_state_values[non_final_mask] = target_net(non_final_next_states).min(1).values\n",
    "#     # Compute the expected Q values\n",
    "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "#     # Compute Huber loss\n",
    "#     criterion = nn.SmoothL1Loss()\n",
    "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "#     # Optimize the model\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     # In-place gradient clipping\n",
    "#     torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no batch version\n",
    "# create state\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = transitions[0]\n",
    "\n",
    "    if batch.next_state is not None:\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        action_batch = torch.Tensor(batch.action)\n",
    "        reward_batch = torch.Tensor(batch.reward)\n",
    "    \n",
    "        # create network input\n",
    "        # inputs shape torch.Size([67, 9])\n",
    "        state_inputs = np.vstack((batch.state, actions))\n",
    "        state_inputs = torch.from_numpy(state_inputs)\n",
    "        state_inputs = state_inputs.to(device, dtype=torch.float32)\n",
    "\n",
    "        next_state_inputs = np.vstack((batch.next_state, actions))\n",
    "        next_state_inputs = torch.from_numpy(next_state_inputs)\n",
    "        next_state_inputs = next_state_inputs.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        # policy_net is the assumed-to-be perfect policy network\n",
    "        state_action_values = policy_net(state_inputs)[action_batch]\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        # target_net is the approximation of the perfect policy network\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # TODO: change max to min to select DRC setting that yields minimum DRC values\n",
    "            next_state_values[0] = target_net(next_state_inputs).min(0).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "        # Compute Huber loss\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq8clj_DEupn"
   },
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and obtain the initial ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop.\n",
    "\n",
    "Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50\n",
    "episodes are scheduled so training does not take too long. However, 50\n",
    "episodes is insufficient for to observe good performance on CartPole.\n",
    "You should see the model constantly achieve 500 steps within 600 training\n",
    "episodes. Training RL agents can be a noisy process, so restarting training\n",
    "can produce better results if convergence is not observed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jG4TnOK4Eupn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 150\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = observation\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKNVv055Eupn"
   },
   "source": [
    "Here is the diagram that illustrates the overall resulting data flow.\n",
    "\n",
    ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
    "\n",
    "Actions are chosen either randomly or based on a policy, getting the next\n",
    "step sample from the gym environment. We record the results in the\n",
    "replay memory and also run optimization step on every iteration.\n",
    "Optimization picks a random batch from the replay memory to do training of the\n",
    "new policy. The \"older\" target_net is also used in optimization to compute the\n",
    "expected Q values. A soft update of its weights are performed at every step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create API for saving and loading memory data\n",
    "def saveMemoryData(memory, path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMemoryData(memory, path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Test pipeline\n",
    "\n",
    "Use trained model to help select optimal actions until finishing 1 trial, and compare a random action selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create actions for network input\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def select_test_action(state, model):\n",
    "    global actions\n",
    "    # select action according to policy\n",
    "    with torch.no_grad():\n",
    "        inputs = np.vstack((state, actions))\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        # select the action with minimum DRC output as optimum action\n",
    "        action = model(inputs).min(0).indices.view(1, 1)\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 action 34 DRC = 915338\n",
      "\n",
      "iteration 1 action 34 DRC = 368508\n",
      "\n",
      "iteration 2 action 34 DRC = 242341\n",
      "\n",
      "iteration 3 action 34 DRC = 128075\n",
      "\n",
      "iteration 4 action 34 DRC = 125669\n",
      "\n",
      "iteration 5 action 34 DRC = 120070\n",
      "\n",
      "iteration 6 action 34 DRC = 89600\n",
      "\n",
      "iteration 7 action 34 DRC = 16144\n",
      "\n",
      "iteration 8 action 34 DRC = 3344\n",
      "\n",
      "iteration 9 action 34 DRC = 2096\n",
      "\n",
      "iteration 10 action 34 DRC = 606\n",
      "\n",
      "iteration 11 action 34 DRC = 189\n",
      "\n",
      "iteration 12 action 34 DRC = 61\n",
      "\n",
      "iteration 13 action 34 DRC = 35\n",
      "\n",
      "iteration 14 action 34 DRC = 13\n",
      "\n",
      "iteration 15 action 34 DRC = 7\n",
      "\n",
      "iteration 16 action 34 DRC = 3\n",
      "\n",
      "iteration 17 action 34 DRC = 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12350/1370369620.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if int(self._curr_drc * self.maxdrc) == 0:\n",
      "/tmp/ipykernel_12350/1370369620.py:136: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "count = 0\n",
    "# start trial\n",
    "env.reset()\n",
    "action = select_test_action(observation, policy_net)\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "while not(terminated or truncated):\n",
    "    print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "    # start next iteration\n",
    "    count += 1\n",
    "    action = select_test_action(observation, policy_net)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Save model. Load model and run test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to torch script via tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to Torch Script via annotation\n",
    "scriptModule = torch.jit.script(policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to a file\n",
    "modelName = \"./model/DQN_FC.pt\"\n",
    "scriptModule.save(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify C++ model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=DQN\n",
       "  (layer1): RecursiveScriptModule(original_name=Linear)\n",
       "  (layer2): RecursiveScriptModule(original_name=Linear)\n",
       "  (layer3): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TorchScript tutorial: https://pytorch.org/tutorials/advanced/cpp_export.html#step-3-loading-your-script-module-in-c\n",
    "# load model\n",
    "loadModelName = modelName\n",
    "model = torch.jit.load(loadModelName)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 action 34 DRC = 632975\n",
      "\n",
      "iteration 1 action 34 DRC = 496529\n",
      "\n",
      "iteration 2 action 34 DRC = 267065\n",
      "\n",
      "iteration 3 action 34 DRC = 195857\n",
      "\n",
      "iteration 4 action 34 DRC = 64767\n",
      "\n",
      "iteration 5 action 34 DRC = 41343\n",
      "\n",
      "iteration 6 action 34 DRC = 2933\n",
      "\n",
      "iteration 7 action 34 DRC = 1601\n",
      "\n",
      "iteration 8 action 34 DRC = 1308\n",
      "\n",
      "iteration 9 action 34 DRC = 556\n",
      "\n",
      "iteration 10 action 34 DRC = 402\n",
      "\n",
      "iteration 11 action 34 DRC = 398\n",
      "\n",
      "iteration 12 action 34 DRC = 235\n",
      "\n",
      "iteration 13 action 34 DRC = 56\n",
      "\n",
      "iteration 14 action 34 DRC = 20\n",
      "\n",
      "iteration 15 action 34 DRC = 12\n",
      "\n",
      "iteration 16 action 34 DRC = 4\n",
      "\n",
      "iteration 17 action 34 DRC = 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12350/1370369620.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if int(self._curr_drc * self.maxdrc) == 0:\n",
      "/tmp/ipykernel_12350/1370369620.py:136: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "count = 0\n",
    "# start trial\n",
    "env.reset()\n",
    "action = select_test_action(observation, model)\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "while not(terminated or truncated):\n",
    "    print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "    # start next iteration\n",
    "    count += 1\n",
    "    action = select_test_action(observation, model)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
