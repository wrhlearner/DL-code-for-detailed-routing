{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YsQPzmYoEupi"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM5DrM8lEupk"
   },
   "source": [
    "\n",
    "# Reinforcement Learning (DQN) Tutorial\n",
    "**Author**: [Adam Paszke](https://github.com/apaszke)\n",
    "            [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n",
    "\n",
    "\n",
    "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\n",
    "on the CartPole-v1 task from [Gymnasium](https://gymnasium.farama.org)_.\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find more\n",
    "information about the environment and other more challenging environments at\n",
    "[Gymnasium's website](https://gymnasium.farama.org/environments/classic_control/cart_pole/)_.\n",
    "\n",
    ".. figure:: /_static/img/cartpole.gif\n",
    "   :alt: CartPole\n",
    "\n",
    "   CartPole\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more than 2.4\n",
    "units away from center. This means better performing scenarios will run\n",
    "for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "We take these 4 inputs without any scaling and pass them through a\n",
    "small fully-connected network with 2 outputs, one for each action.\n",
    "The network is trained to predict the expected value for each action,\n",
    "given the input state. The action with the highest expected value is\n",
    "then chosen.\n",
    "\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "[gymnasium](https://gymnasium.farama.org/)_ for the environment,\n",
    "installed by using `pip`. This is a fork of the original OpenAI\n",
    "Gym project and maintained by the same team since Gym v0.19.\n",
    "If you are running this in Google Colab, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uUnFXf2REupk"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip3 install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dha1qo8BEupl"
   },
   "source": [
    "We'll also use the following from PyTorch:\n",
    "\n",
    "-  neural networks (``torch.nn``)\n",
    "-  optimization (``torch.optim``)\n",
    "-  automatic differentiation (``torch.autograd``)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GZUdaeq7Eupl"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import DREnv_fake\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DREnv(gym.Env):\n",
    "    # draw bar chart and line curve for visuliazation\n",
    "    # metadata = (\"render_modes\": [])\n",
    "\n",
    "    def __init__(self, actions, window_size=2, render_mode=None, drcmax=1e6, input_size=9):\n",
    "        self.size = drcmax\n",
    "        self.maxdrc = None\n",
    "        self.window_size = window_size\n",
    "        self.input_size = input_size\n",
    "        self._index = -1\n",
    "        if actions is None:\n",
    "            # index: (size, offset, mazeEndIter, DRCCost, MarkerCost, FixedShapeCost, Decay, ripupMode, followGuide)\n",
    "            # We have 65 actions, corresponding to default DRC sequences\n",
    "            # shapeCost = 8\n",
    "            # MARKERCOST = 32\n",
    "            # ripupMode: ALL->0, DRC->0.5, NEARDRC->1\n",
    "            # followGuide: True->1, False->0\n",
    "            actions = [[7,  0,  3,      8,       0,       8, 0.950, 0    ,  1],\n",
    "                        [7, -2,  3,      8,       8,       8, 0.950, 0    ,  1],\n",
    "                        [7, -5,  3,      8,       8,       8, 0.950, 0    ,  1],\n",
    "                        [7,  0,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,      8,      32,   2 * 8, 0.950, 0.5    , 0],\n",
    "                        [7,  0,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,  2 * 8,      32,   3 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,  2 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,  2 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,   4 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8,  4 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  4 * 8,      32,   4 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8,      8,      32,  10 * 8, 0.950, 1, 0],\n",
    "                        [7, -6,  8,  4 * 8,      32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [5, -2,  8,      8,      32,  10 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,  8 * 8,  2 * 32,  10 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -4,  8,      8,      32,  50 * 8, 0.950, 1, 0],\n",
    "                        [7, -5,  8,  8 * 8,  2 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8,  8 * 8,  2 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [3, -1,  8,      8,      32,  50 * 8, 0.950, 0    , 0],\n",
    "                        [7,  0,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -1,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -2,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -3,  8,      8,      32,  50 * 8, 0.950, 1, 0],\n",
    "                        [7, -4,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -5,  8, 16 * 8,  4 * 32,  50 * 8, 0.950, 0.5    , 0],\n",
    "                        [7, -6,  8, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [3, -2,  8,      8,      32, 100 * 8, 0.990, 0    , 0],\n",
    "                        [7,  0, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -1, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -2, 16,      8,      32, 100 * 8, 0.990, 1, 0],\n",
    "                        [7, -3, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -4, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -5, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [7, -6, 16, 16 * 8,  4 * 32, 100 * 8, 0.990, 0.5    , 0],\n",
    "                        [3, -0,  8,      8,      32, 100 * 8, 0.990, 0    , 0],\n",
    "                        [7,  0, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -1, 32,      8,      32, 100 * 8, 0.999, 1, 0],\n",
    "                        [7, -2, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -3, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -4, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -5, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -6, 32, 32 * 8,  8 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [3, -1,  8,      8,      32, 100 * 8, 0.999, 0    , 0],\n",
    "                        [7,  0, 64,      8,      32, 100 * 8, 0.999, 1, 0],\n",
    "                        [7, -1, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -2, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -3, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -4, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -5, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0],\n",
    "                        [7, -6, 64, 64 * 8, 16 * 32, 100 * 8, 0.999, 0.5    , 0]]\n",
    "        columns = ['size', 'offset', 'mazeEndIter', 'DRCCost', 'MarkerCost', 'FixedShapeCost', 'Decay', 'ripupMode', 'followGuide']\n",
    "        self.actions = pd.DataFrame(actions, columns=columns)\n",
    "        self.action_space = spaces.Discrete(self.actions.shape[0])\n",
    "        # Observations are boxes containing the historical DRC sequence settings and DRC values\n",
    "        self.observations = None\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.window_size, self.input_size), dtype=float)\n",
    "\n",
    "    def _preprocessing(self):\n",
    "        if 'size' in self.actions.columns:\n",
    "            # action space preprocessing\n",
    "            # drop columns: size\n",
    "            self.actions = self.actions.drop(columns=['size'])\n",
    "    \n",
    "            # normalize coefficients\n",
    "            for column in ['offset', 'mazeEndIter', 'DRCCost', 'MarkerCost', 'FixedShapeCost', 'Decay']:\n",
    "                sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "                self.actions[column] = sc.fit_transform(self.actions[column].to_numpy().reshape(-1, 1))\n",
    "        \n",
    "        # observation space preprocessing\n",
    "        # padding historical data\n",
    "        self.observations = np.ones((self.window_size, self.input_size), dtype=float) * -1\n",
    "        # generate drc value before 0th iteration\n",
    "        self.maxdrc = self.np_random.integers(1, self.size, size=1, dtype=int)[0]\n",
    "        self.observations[-1, -1] = 1\n",
    "        # parameters initialization\n",
    "        self._index = -1\n",
    "        self._reward = -1   # reward starting value\n",
    "        self._curr_drc = 1\n",
    "\n",
    "    def _action_to_setting(self, action):\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the setting we will use for next iteration.\n",
    "        I.e. 0 corresponds to the 0th default setting\n",
    "        \"\"\"\n",
    "        # return selected value\n",
    "        return list(self.actions.iloc[action, :])\n",
    "\n",
    "    def _get_obs(self, action):\n",
    "        \"\"\"get DRC value for current iteration\"\"\"\n",
    "        if action:\n",
    "            # update DRC value\n",
    "            # randomly generate DRC value for current iteration\n",
    "            if int(self._curr_drc * self.maxdrc) == 0:\n",
    "                drc = 0\n",
    "            else:\n",
    "                drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n",
    "            # print(f\"self._curr_drc {self._curr_drc}, self.maxdrc {self.maxdrc}\")\n",
    "            self._curr_drc = drc\n",
    "            # generate observation for current iteration\n",
    "            setting = self._action_to_setting(action)\n",
    "            line = np.append(setting, drc)\n",
    "            # update the whole observation\n",
    "            self.observations = np.r_[self.observations, [line]]\n",
    "            self.observations = np.delete(self.observations, 0, 0)\n",
    "            \n",
    "        return self.observations\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"\"provide index for current iteration\"\"\"\n",
    "        return {\n",
    "            \"iteration index\": self._index\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._preprocessing()\n",
    "        observation = self._get_obs(None)\n",
    "        info = self._get_info()\n",
    "    \n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self._render_frame()\n",
    "    \n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the setting we use\n",
    "        reward = self._reward\n",
    "        \n",
    "        # update index\n",
    "        self._index += 1\n",
    "\n",
    "        # update observation\n",
    "        observation = self._get_obs(action)\n",
    "\n",
    "        # update truncated\n",
    "        if self._index == 64 and self._curr_drc != 0:\n",
    "            # punish truncated case\n",
    "            truncated = True\n",
    "            reward = -255    # -255 is a large value without detailed thought\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        # An episode is done iff current DRC value goes to zero\n",
    "        if self._curr_drc:\n",
    "            terminated = False\n",
    "        else:\n",
    "            terminated = True\n",
    "            # udpate reward: encourage finishing detailed routing using less iterations\n",
    "            reward = 0\n",
    "\n",
    "        # update info\n",
    "        info = self._get_info()\n",
    "    \n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self._render_frame()\n",
    "    \n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in notebook\n",
    "actions = None\n",
    "window_size = 2\n",
    "input_size = 9\n",
    "env = DREnv(actions, window_size=window_size, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>-4</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>7</td>\n",
       "      <td>-6</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>800</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    size  offset  mazeEndIter  DRCCost  MarkerCost  FixedShapeCost  Decay  \\\n",
       "0      7       0            3        8           0               8  0.950   \n",
       "1      7      -2            3        8           8               8  0.950   \n",
       "2      7      -5            3        8           8               8  0.950   \n",
       "3      7       0            8        8          32              16  0.950   \n",
       "4      7      -1            8        8          32              16  0.950   \n",
       "..   ...     ...          ...      ...         ...             ...    ...   \n",
       "60     7      -2           64      512         512             800  0.999   \n",
       "61     7      -3           64      512         512             800  0.999   \n",
       "62     7      -4           64      512         512             800  0.999   \n",
       "63     7      -5           64      512         512             800  0.999   \n",
       "64     7      -6           64      512         512             800  0.999   \n",
       "\n",
       "    ripupMode  followGuide  \n",
       "0         0.0            1  \n",
       "1         0.0            1  \n",
       "2         0.0            1  \n",
       "3         0.5            0  \n",
       "4         0.5            0  \n",
       "..        ...          ...  \n",
       "60        0.5            0  \n",
       "61        0.5            0  \n",
       "62        0.5            0  \n",
       "63        0.5            0  \n",
       "64        0.5            0  \n",
       "\n",
       "[65 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iteration index': -1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.96875</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.96875</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.87500</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.87500</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      offset  mazeEndIter  DRCCost  MarkerCost  FixedShapeCost  Decay  \\\n",
       "0   1.000000    -1.000000     -1.0    -1.00000       -1.000000   -1.0   \n",
       "1   0.333333    -1.000000     -1.0    -0.96875       -1.000000   -1.0   \n",
       "2  -0.666667    -1.000000     -1.0    -0.96875       -1.000000   -1.0   \n",
       "3   1.000000    -0.836066     -1.0    -0.87500       -0.979798   -1.0   \n",
       "4   0.666667    -0.836066     -1.0    -0.87500       -0.979798   -1.0   \n",
       "..       ...          ...      ...         ...             ...    ...   \n",
       "60  0.333333     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "61  0.000000     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "62 -0.333333     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "63 -0.666667     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "64 -1.000000     1.000000      1.0     1.00000        1.000000    1.0   \n",
       "\n",
       "    ripupMode  followGuide  \n",
       "0         0.0            1  \n",
       "1         0.0            1  \n",
       "2         0.0            1  \n",
       "3         0.5            0  \n",
       "4         0.5            0  \n",
       "..        ...          ...  \n",
       "60        0.5            0  \n",
       "61        0.5            0  \n",
       "62        0.5            0  \n",
       "63        0.5            0  \n",
       "64        0.5            0  \n",
       "\n",
       "[65 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   offset          65 non-null     float64\n",
      " 1   mazeEndIter     65 non-null     float64\n",
      " 2   DRCCost         65 non-null     float64\n",
      " 3   MarkerCost      65 non-null     float64\n",
      " 4   FixedShapeCost  65 non-null     float64\n",
      " 5   Decay           65 non-null     float64\n",
      " 6   ripupMode       65 non-null     float64\n",
      " 7   followGuide     65 non-null     int64  \n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 4.2 KB\n"
     ]
    }
   ],
   "source": [
    "env.actions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>mazeEndIter</th>\n",
       "      <th>DRCCost</th>\n",
       "      <th>MarkerCost</th>\n",
       "      <th>FixedShapeCost</th>\n",
       "      <th>Decay</th>\n",
       "      <th>ripupMode</th>\n",
       "      <th>followGuide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.056410</td>\n",
       "      <td>-0.532913</td>\n",
       "      <td>-0.603907</td>\n",
       "      <td>-0.545192</td>\n",
       "      <td>-0.040559</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.046154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.665544</td>\n",
       "      <td>0.593202</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>0.565018</td>\n",
       "      <td>0.886749</td>\n",
       "      <td>0.915474</td>\n",
       "      <td>0.240942</td>\n",
       "      <td>0.211451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.939394</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.836066</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.010101</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.573770</td>\n",
       "      <td>-0.523810</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offset  mazeEndIter    DRCCost  MarkerCost  FixedShapeCost  \\\n",
       "count  65.000000    65.000000  65.000000   65.000000       65.000000   \n",
       "mean    0.056410    -0.532913  -0.603907   -0.545192       -0.040559   \n",
       "std     0.665544     0.593202   0.595453    0.565018        0.886749   \n",
       "min    -1.000000    -1.000000  -1.000000   -1.000000       -1.000000   \n",
       "25%    -0.666667    -0.836066  -1.000000   -0.875000       -0.939394   \n",
       "50%     0.000000    -0.836066  -0.904762   -0.875000       -0.010101   \n",
       "75%     0.666667    -0.573770  -0.523810   -0.500000        1.000000   \n",
       "max     1.000000     1.000000   1.000000    1.000000        1.000000   \n",
       "\n",
       "           Decay  ripupMode  followGuide  \n",
       "count  65.000000  65.000000    65.000000  \n",
       "mean   -0.287284   0.476923     0.046154  \n",
       "std     0.915474   0.240942     0.211451  \n",
       "min    -1.000000   0.000000     0.000000  \n",
       "25%    -1.000000   0.500000     0.000000  \n",
       "50%    -1.000000   0.500000     0.000000  \n",
       "75%     0.632653   0.500000     0.000000  \n",
       "max     1.000000   1.000000     1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design 1 iteration 0 DRC: 0.19460836496035053, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.19460836]]\n",
      "Design 1 iteration 1 DRC: 0.028298466475361782, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.19460836]\n",
      " [ 0.66666667 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.02829847]]\n",
      "Design 1 iteration 2 DRC: 0.024568792619334135, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.02829847]\n",
      " [-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.02456879]]\n",
      "Design 1 iteration 3 DRC: 0.013447267734271344, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.02456879]\n",
      " [ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.01344727]]\n",
      "Design 1 iteration 4 DRC: 0.01163642815267364, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.01344727]\n",
      " [-0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.01163643]]\n",
      "Design 1 iteration 5 DRC: 0.0069926563608874075, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.01163643]\n",
      " [ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00699266]]\n",
      "Design 1 iteration 6 DRC: 0.006348545157209417, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00699266]\n",
      " [-0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.00634855]]\n",
      "Design 1 iteration 7 DRC: 0.004354500293128452, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.00634855]\n",
      " [ 0.66666667 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.0043545 ]]\n",
      "Design 1 iteration 8 DRC: 0.001095374741584128, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.0043545 ]\n",
      " [ 0.66666667 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.00109537]]\n",
      "Design 1 iteration 9 DRC: 0.0006556820636243019, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.09537474e-03]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.55682064e-04]]\n",
      "Design 1 iteration 10 DRC: 0.0006036131938659014, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.55682064e-04]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.03613194e-04]]\n",
      "Design 1 iteration 11 DRC: 0.00041462248140948505, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.03613194e-04]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.14622481e-04]]\n",
      "Design 1 iteration 12 DRC: 0.0003586966583356475, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.14622481e-04]\n",
      " [ 3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.58696658e-04]]\n",
      "Design 1 iteration 13 DRC: 0.00033362646178530656, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.58696658e-04]\n",
      " [-3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.33626462e-04]]\n",
      "Design 1 iteration 14 DRC: 3.85695331543707e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.33626462e-04]\n",
      " [-1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.85695332e-06]]\n",
      "Design 1 iteration 15 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.85695332e-06]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 2 iteration 0 DRC: 0.35599042565994454, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.35599043]]\n",
      "Design 2 iteration 1 DRC: 0.16773370140955562, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.35599043]\n",
      " [-1.         -0.83606557 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.1677337 ]]\n",
      "Design 2 iteration 2 DRC: 0.0477216369090512, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.1677337 ]\n",
      " [ 0.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.04772164]]\n",
      "Design 2 iteration 3 DRC: 0.03397820961029583, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.04772164]\n",
      " [-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.03397821]]\n",
      "Design 2 iteration 4 DRC: 0.0334893893312374, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.03397821]\n",
      " [ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.03348939]]\n",
      "Design 2 iteration 5 DRC: 0.008419892162383423, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.03348939]\n",
      " [ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00841989]]\n",
      "Design 2 iteration 6 DRC: 0.002710055285722139, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00841989]\n",
      " [-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.00271006]]\n",
      "Design 2 iteration 7 DRC: 0.0006864284769756734, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.71005529e-03]\n",
      " [-3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.86428477e-04]]\n",
      "Design 2 iteration 8 DRC: 0.00041750303469732515, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   6.86428477e-04]\n",
      " [-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.17503035e-04]]\n",
      "Design 2 iteration 9 DRC: 0.0001203478498593713, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   4.17503035e-04]\n",
      " [-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.20347850e-04]]\n",
      "Design 2 iteration 10 DRC: 8.914655545138616e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   1.20347850e-04]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   8.91465555e-06]]\n",
      "Design 2 iteration 11 DRC: 4.457327772569308e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.59595960e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   8.91465555e-06]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.45732777e-06]]\n",
      "Design 2 iteration 12 DRC: 2.9715518483795384e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.45732777e-06]\n",
      " [-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.97155185e-06]]\n",
      "Design 2 iteration 13 DRC: 1.4857759241897692e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   2.97155185e-06]\n",
      " [-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.48577592e-06]]\n",
      "Design 2 iteration 14 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -9.68253968e-01 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.48577592e-06]\n",
      " [ 3.33333333e-01 -5.73770492e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 3 iteration 0 DRC: 0.36132719849971856, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-1.         -0.83606557 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.3613272 ]]\n",
      "Design 3 iteration 1 DRC: 0.11385081244761673, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.3613272 ]\n",
      " [-0.66666667 -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.11385081]]\n",
      "Design 3 iteration 2 DRC: 0.08639767780817338, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.52380952 -0.5        -0.01010101 -1.\n",
      "   0.5         0.          0.11385081]\n",
      " [ 0.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.08639768]]\n",
      "Design 3 iteration 3 DRC: 1.228933015777042e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   8.63976778e-02]\n",
      " [-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.22893302e-06]]\n",
      "Design 3 iteration 4 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-3.33333333e-01 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.22893302e-06]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 4 iteration 0 DRC: 0.91302947064006, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.91302947]]\n",
      "Design 4 iteration 1 DRC: 0.5623790888394513, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.91302947]\n",
      " [ 0.         -0.83606557 -1.         -0.875      -0.93939394 -1.\n",
      "   0.          0.          0.56237909]]\n",
      "Design 4 iteration 2 DRC: 0.03969626963780475, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -1.         -0.875      -0.93939394 -1.\n",
      "   0.          0.          0.56237909]\n",
      " [ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.03969627]]\n",
      "Design 4 iteration 3 DRC: 0.03001116161747259, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.03969627]\n",
      " [-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.03001116]]\n",
      "Design 4 iteration 4 DRC: 0.018861759321324795, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.03001116]\n",
      " [-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.01886176]]\n",
      "Design 4 iteration 5 DRC: 0.0029805030515037646, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.01886176]\n",
      " [ 0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.0029805 ]]\n",
      "Design 4 iteration 6 DRC: 0.001456659790540262, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.57377049 -0.52380952 -0.5         1.          0.63265306\n",
      "   0.5         0.          0.0029805 ]\n",
      " [ 0.         -0.83606557 -1.         -0.875      -0.93939394 -1.\n",
      "   0.          0.          0.00145666]]\n",
      "Design 4 iteration 7 DRC: 0.00112532312958928, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -1.         -0.875      -0.93939394 -1.\n",
      "   0.          0.          0.00145666]\n",
      " [-0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00112532]]\n",
      "Design 4 iteration 8 DRC: 0.0004702842929626842, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.12532313e-03]\n",
      " [-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.70284293e-04]]\n",
      "Design 4 iteration 9 DRC: 2.595724993625205e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.70284293e-04]\n",
      " [ 1.00000000e+00  1.00000000e+00 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   2.59572499e-05]]\n",
      "Design 4 iteration 10 DRC: 7.63448527536825e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00  1.00000000e+00 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   2.59572499e-05]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  0.00000000e+00  0.00000000e+00\n",
      "   7.63448528e-06]]\n",
      "Design 4 iteration 11 DRC: 3.0537941101472998e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  0.00000000e+00  0.00000000e+00\n",
      "   7.63448528e-06]\n",
      " [-1.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.05379411e-06]]\n",
      "Design 4 iteration 12 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-1.00000000e+00 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   3.05379411e-06]\n",
      " [ 6.66666667e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 5 iteration 0 DRC: 0.7572309766886706, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.75723098]]\n",
      "Design 5 iteration 1 DRC: 0.4693483579111372, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.75723098]\n",
      " [-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.46934836]]\n",
      "Design 5 iteration 2 DRC: 0.2788772112051096, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.46934836]\n",
      " [-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.27887721]]\n",
      "Design 5 iteration 3 DRC: 0.13326226498471574, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.27887721]\n",
      " [ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.13326226]]\n",
      "Design 5 iteration 4 DRC: 0.09747639027456984, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.13326226]\n",
      " [ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.09747639]]\n",
      "Design 5 iteration 5 DRC: 0.09452318149307311, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.09747639]\n",
      " [ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.09452318]]\n",
      "Design 5 iteration 6 DRC: 0.017755697995963684, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.09452318]\n",
      " [-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.0177557 ]]\n",
      "Design 5 iteration 7 DRC: 0.00402720642166309, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.66666667 -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.0177557 ]\n",
      " [-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00402721]]\n",
      "Design 5 iteration 8 DRC: 0.0016502890568410118, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00402721]\n",
      " [ 0.33333333 -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00165029]]\n",
      "Design 5 iteration 9 DRC: 0.00043962151548697766, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.65028906e-03]\n",
      " [ 0.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.39621515e-04]]\n",
      "Design 5 iteration 10 DRC: 1.822265349168819e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.39621515e-04]\n",
      " [-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.82226535e-05]]\n",
      "Design 5 iteration 11 DRC: 5.6945792161525604e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.82226535e-05]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.69457922e-06]]\n",
      "Design 5 iteration 12 DRC: 4.555663372922048e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.69457922e-06]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.55566337e-06]]\n",
      "Design 5 iteration 13 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   4.55566337e-06]\n",
      " [ 3.33333333e-01 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 6 iteration 0 DRC: 1.0, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1.  1.]]\n",
      "Design 6 iteration 1 DRC: 0.24811661155068399, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.24811661]]\n",
      "Design 6 iteration 2 DRC: 0.14978443792457838, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.         -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.24811661]\n",
      " [-1.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.14978444]]\n",
      "Design 6 iteration 3 DRC: 0.136700071017123, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.14978444]\n",
      " [ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.13670007]]\n",
      "Design 6 iteration 4 DRC: 0.05558560432702309, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.83606557 -0.96825397 -0.875      -0.95959596 -1.\n",
      "   0.5         0.          0.13670007]\n",
      " [-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.0555856 ]]\n",
      "Design 6 iteration 5 DRC: 0.05364590432056699, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.0555856 ]\n",
      " [ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.0536459 ]]\n",
      "Design 6 iteration 6 DRC: 0.038426719654526806, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.0536459 ]\n",
      " [ 0.          1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.03842672]]\n",
      "Design 6 iteration 7 DRC: 0.0053313056390465055, reward: -1, terminated: False, truncated: False\n",
      "observation [[0.         1.         1.         1.         1.         1.\n",
      "  0.5        0.         0.03842672]\n",
      " [0.66666667 1.         1.         1.         1.         1.\n",
      "  0.5        0.         0.00533131]]\n",
      "Design 6 iteration 8 DRC: 0.0010688435686462988, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00533131]\n",
      " [ 0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00106884]]\n",
      "Design 6 iteration 9 DRC: 0.0010688435686462988, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.00533131]\n",
      " [ 0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.00106884]]\n",
      "Design 6 iteration 10 DRC: 5.738757415550598e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 3.33333333e-01 -8.36065574e-01 -9.04761905e-01 -8.75000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.06884357e-03]\n",
      " [ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.73875742e-05]]\n",
      "Design 6 iteration 11 DRC: 5.021412738606773e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.79797980e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.73875742e-05]\n",
      " [-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   5.02141274e-05]]\n",
      "Design 6 iteration 12 DRC: 1.1477514831101196e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   5.02141274e-05]\n",
      " [-1.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.14775148e-05]]\n",
      "Design 6 iteration 13 DRC: 1.0042825477213546e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.00000000e+00 -4.91803279e-02 -1.58730159e-02  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.14775148e-05]\n",
      " [-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.00428255e-05]]\n",
      "Design 6 iteration 14 DRC: 5.738757415550598e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.00428255e-05]\n",
      " [-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   5.73875742e-06]]\n",
      "Design 6 iteration 15 DRC: 2.869378707775299e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   5.73875742e-06]\n",
      " [-6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.86937871e-06]]\n",
      "Design 6 iteration 16 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[-6.66666667e-01 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.86937871e-06]\n",
      " [-6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 7 iteration 0 DRC: 0.3864829726029118, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [ 0.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.38648297]]\n",
      "Design 7 iteration 1 DRC: 0.05841194869797882, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.04918033 -0.01587302  0.          1.          1.\n",
      "   0.5         0.          0.38648297]\n",
      " [ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.05841195]]\n",
      "Design 7 iteration 2 DRC: 0.045445124172497395, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -1.         -1.         -0.96875    -1.         -1.\n",
      "   0.          1.          0.05841195]\n",
      " [ 0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.04544512]]\n",
      "Design 7 iteration 3 DRC: 0.015253055635810921, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333  1.          1.          1.          1.          1.\n",
      "   0.5         0.          0.04544512]\n",
      " [ 0.66666667 -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01525306]]\n",
      "Design 7 iteration 4 DRC: 0.0011054304269725024, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -0.9047619  -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.01525306]\n",
      " [ 1.         -0.83606557 -0.77777778 -0.75       -0.81818182 -1.\n",
      "   0.5         0.          0.00110543]]\n",
      "Design 7 iteration 5 DRC: 0.0005118896579446532, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 1.00000000e+00 -8.36065574e-01 -7.77777778e-01 -7.50000000e-01\n",
      "  -8.18181818e-01 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.10543043e-03]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.11889658e-04]]\n",
      "Design 7 iteration 6 DRC: 0.0002889193161405404, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   5.11889658e-04]\n",
      " [ 6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.88919316e-04]]\n",
      "Design 7 iteration 7 DRC: 0.0001790043589131609, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 6.66666667e-01 -8.36065574e-01 -5.23809524e-01 -5.00000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   2.88919316e-04]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.79004359e-04]]\n",
      "Design 7 iteration 8 DRC: 0.0001476000854196239, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -9.39393939e-01 -1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.79004359e-04]\n",
      " [-3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.47600085e-04]]\n",
      "Design 7 iteration 9 DRC: 1.57021367467685e-05, reward: -1, terminated: False, truncated: False\n",
      "observation [[-3.33333333e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  5.00000000e-01  0.00000000e+00\n",
      "   1.47600085e-04]\n",
      " [-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.57021367e-05]]\n",
      "Design 7 iteration 10 DRC: 3.1404273493537002e-06, reward: -1, terminated: False, truncated: False\n",
      "observation [[-6.66666667e-01 -1.00000000e+00 -1.00000000e+00 -9.68750000e-01\n",
      "  -1.00000000e+00 -1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   1.57021367e-05]\n",
      " [ 6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   3.14042735e-06]]\n",
      "Design 7 iteration 11 DRC: 0.0, reward: 0, terminated: True, truncated: False\n",
      "observation [[ 6.66666667e-01 -5.73770492e-01 -5.23809524e-01 -5.00000000e-01\n",
      "   1.00000000e+00  6.32653061e-01  5.00000000e-01  0.00000000e+00\n",
      "   3.14042735e-06]\n",
      " [ 0.00000000e+00 -8.36065574e-01 -1.00000000e+00 -8.75000000e-01\n",
      "  -1.01010101e-02 -1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Design 8 iteration 0 DRC: 0.8939368839559347, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.          1.        ]\n",
      " [-1.         -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.89393688]]\n",
      "Design 8 iteration 1 DRC: 0.3758075871780005, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -0.96825397 -0.875      -0.93939394 -1.\n",
      "   0.5         0.          0.89393688]\n",
      " [ 0.33333333 -0.57377049 -1.         -0.875       1.          0.63265306\n",
      "   1.          0.          0.37580759]]\n",
      "Design 8 iteration 2 DRC: 0.32422036776277646, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.33333333 -0.57377049 -1.         -0.875       1.          0.63265306\n",
      "   1.          0.          0.37580759]\n",
      " [-0.33333333 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.32422037]]\n",
      "Design 8 iteration 3 DRC: 0.03376335624948232, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.32422037]\n",
      " [ 0.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.03376336]]\n",
      "Design 8 iteration 4 DRC: 0.02767539136917088, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.03376336]\n",
      " [ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.02767539]]\n",
      "Design 8 iteration 5 DRC: 0.02060382672078191, reward: -1, terminated: False, truncated: False\n",
      "observation [[ 0.66666667 -0.83606557 -1.         -0.875       1.          1.\n",
      "   0.          0.          0.02767539]\n",
      " [-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.02060383]]\n",
      "Design 8 iteration 6 DRC: 0.01796881471051106, reward: -1, terminated: False, truncated: False\n",
      "observation [[-1.         -0.83606557 -1.         -0.875      -0.97979798 -1.\n",
      "   0.5         0.          0.02060383]\n",
      " [-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.01796881]]\n",
      "Design 8 iteration 7 DRC: 0.009173362047544107, reward: -1, terminated: False, truncated: False\n",
      "observation [[-0.33333333 -0.83606557 -0.9047619  -0.875      -0.81818182 -1.\n",
      "   0.5         0.          0.01796881]\n",
      " [-0.66666667 -0.83606557 -1.         -0.875      -0.81818182 -1.\n",
      "   1.          0.          0.00917336]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15530/1370369620.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if int(self._curr_drc * self.maxdrc) == 0:\n",
      "/tmp/ipykernel_15530/1370369620.py:136: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "sample = 100\n",
    "num = 1\n",
    "env.reset()\n",
    "\n",
    "for _ in range(sample):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"Design {} iteration {} DRC: {}, reward: {}, terminated: {}, truncated: {}\\nobservation {}\".format(num, \n",
    "        info['iteration index'], observation[-1, -1], reward, terminated, truncated, observation))\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        num += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EEcLRIAEupl"
   },
   "source": [
    "## Replay Memory\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classes:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment. It essentially maps (state, action) pairs\n",
    "   to their (next_state, reward) result, with the state being the\n",
    "   screen difference image as described later on.\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o03vx5sMEupl"
   },
   "outputs": [],
   "source": [
    "# a data structure containing several named elements\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCS1wkgyEupm"
   },
   "source": [
    "Now, let's define our model. But first, let's quickly recap what a DQN is.\n",
    "\n",
    "## DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. A lower $\\gamma$ makes\n",
    "rewards from the uncertain far future less important for our agent\n",
    "than the ones in the near future that it can be fairly confident\n",
    "about. It also encourages agents to collect reward closer in time\n",
    "than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\\end{align}\n",
    "\n",
    "To minimize this error, we will use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss)_. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "### Q-network\n",
    "\n",
    "Our model will be a feed forward  neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/img/RLLSTM model structure.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1XHTvTBFEupm"
   },
   "outputs": [],
   "source": [
    "# Q-network model\n",
    "# the target is to learn the Q-function, which takes \n",
    "#     state/observation X action pairs as input,\n",
    "#     and make predictions about return (cumulation of rewards) \n",
    "#     Q:State x Action -> Return\n",
    "# inputs:\n",
    "#     n_observations: number of elements in observations, \n",
    "#         which is time series input for current state \n",
    "#         with historical data\n",
    "#     n_actions: number of actions in environment action space\n",
    "# outputs:\n",
    "#     model output with dimension n_actions: each represents the \n",
    "#         expected return of the state x action pair\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.layer1 = nn.Linear(n_observations, 128)\n",
    "#         self.layer2 = nn.Linear(128, 128)\n",
    "#         self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.layer1(x))\n",
    "#         x = F.relu(self.layer2(x))\n",
    "#         return self.layer3(x)\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self,input_size=9, seq_len=2, coeff_size=8, hidden_size=10,\n",
    "#                  output_size=65, num_layers=10,dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.seq_len = seq_len\n",
    "#         self.coeff_size = coeff_size\n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#         # initializing h0 and c0:\n",
    "#         self.hidden = (torch.zeros(num_layers, self.hidden_size).to(device),    # h0\n",
    "#                        torch.zeros(num_layers, self.hidden_size).to(device))    # c0\n",
    "\n",
    "#         # add an LSTM layer:\n",
    "#         self.lstm = nn.LSTM(input_size,hidden_size, \n",
    "#                             num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "#         # TODO: change fully-connected layer structure\n",
    "#         # add a fully-connected layer:\n",
    "#         self.in_features = seq_len * hidden_size + output_size * coeff_size\n",
    "#         self.linear = nn.Linear(self.in_features, self.output_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # TODO: parse inputs\n",
    "#         seq = inputs[:self.seq_len, :].view(inputs.shape[0] - self.output_size, -1).to(device)\n",
    "#         coeff = inputs[self.seq_len:, :self.coeff_size].reshape((inputs.shape[0] - self.seq_len) * self.coeff_size).to(device)\n",
    "        \n",
    "#         lstm_out, self.hidden = self.lstm(seq, self.hidden)\n",
    "        \n",
    "#         data = torch.cat((lstm_out.view(lstm_out.shape[0] * lstm_out.shape[1]), coeff), 0)\n",
    "        \n",
    "#         pred = self.linear(data)\n",
    "        \n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple CNN\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, output_size)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        \n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VkObo6VEupm"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Hyperparameters and utilities\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the duration of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gFR0vqAlEupm"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 1\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSTM-based DQN\n",
    "# # instantiate\n",
    "# seq_len = len(observation)\n",
    "# coeff_size = len(env.actions.columns) - 1\n",
    "# hidden_size = 10         # from LSTM notebook experience\n",
    "# output_size = n_actions\n",
    "# num_layers = 10          # from LSTM notebook experience\n",
    "# dropout = 0.2\n",
    "# policy_net = DQN(input_size=input_size, seq_len=seq_len, coeff_size=coeff_size, \n",
    "#                  hidden_size=hidden_size, output_size=output_size, \n",
    "#                  num_layers=num_layers,dropout=dropout).to(device)\n",
    "# target_net = DQN(input_size=input_size, seq_len=seq_len, coeff_size=coeff_size, \n",
    "#                  hidden_size=hidden_size, output_size=output_size, \n",
    "#                  num_layers=num_layers,dropout=dropout).to(device)\n",
    "\n",
    "# ANN-based DQN\n",
    "# instantiate\n",
    "seq_len = len(observation)\n",
    "output_size = n_actions\n",
    "\n",
    "policy_net = DQN(input_size=input_size * (seq_len + n_actions), output_size=output_size).to(device)\n",
    "target_net = DQN(input_size=input_size * (seq_len + n_actions), output_size=output_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "memory = ReplayMemory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "# create actions for network input\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    global actions\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        # select action according to policy\n",
    "        with torch.no_grad():\n",
    "            # t.min(0) will return the smallest drc value of the tensor returned by the policy network.\n",
    "            # we pick action with the smallest expected DRC value.\n",
    "            # create network input\n",
    "            # inputs shape torch.Size([67, 9])\n",
    "            inputs = np.vstack((state, actions))\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            # select the action with minimum DRC output as optimum action\n",
    "            return policy_net(inputs).min(0).indices.view(1, 1)\n",
    "    else:\n",
    "        # select action randomly from action space\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _  = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    select_action(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udnRhi1vEupm"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By definition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network is updated at every step with a\n",
    "[soft update](https://arxiv.org/pdf/1509.02971.pdf)_ controlled by\n",
    "the hyperparameter ``TAU``, which was previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1uDRpRM7Eupm"
   },
   "outputs": [],
   "source": [
    "# batch version\n",
    "# def optimize_model():\n",
    "#     if len(memory) < BATCH_SIZE:\n",
    "#         return\n",
    "#     transitions = memory.sample(BATCH_SIZE)\n",
    "#     # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "#     # detailed explanation). This converts batch-array of Transitions\n",
    "#     # to Transition of batch-arrays.\n",
    "#     batch = Transition(*zip(*transitions))\n",
    "\n",
    "#     # Compute a mask of non-final states and concatenate the batch elements\n",
    "#     # (a final state would've been the one after which simulation ended)\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                                 if s is not None])\n",
    "#     state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(batch.action)\n",
    "#     reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "#     # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "#     # columns of actions taken. These are the actions which would've been taken\n",
    "#     # for each batch state according to policy_net\n",
    "#     state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "#     # Compute V(s_{t+1}) for all next states.\n",
    "#     # Expected values of actions for non_final_next_states are computed based\n",
    "#     # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "#     # This is merged based on the mask, such that we'll have either the expected\n",
    "#     # state value or 0 in case the state was final.\n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "#     with torch.no_grad():\n",
    "#         # TODO: change max to min to select DRC setting that yields minimum DRC values\n",
    "#         # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "#         next_state_values[non_final_mask] = target_net(non_final_next_states).min(1).values\n",
    "#     # Compute the expected Q values\n",
    "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "#     # Compute Huber loss\n",
    "#     criterion = nn.SmoothL1Loss()\n",
    "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "#     # Optimize the model\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     # In-place gradient clipping\n",
    "#     torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no batch version\n",
    "# create state\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = transitions[0]\n",
    "\n",
    "    if batch.next_state is not None:\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        action_batch = torch.Tensor(batch.action)\n",
    "        reward_batch = torch.Tensor(batch.reward)\n",
    "    \n",
    "        # create network input\n",
    "        # inputs shape torch.Size([67, 9])\n",
    "        state_inputs = np.vstack((batch.state, actions))\n",
    "        state_inputs = torch.from_numpy(state_inputs)\n",
    "        state_inputs = state_inputs.to(device, dtype=torch.float32)\n",
    "\n",
    "        next_state_inputs = np.vstack((batch.next_state, actions))\n",
    "        next_state_inputs = torch.from_numpy(next_state_inputs)\n",
    "        next_state_inputs = next_state_inputs.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        # policy_net is the assumed-to-be perfect policy network\n",
    "        state_action_values = policy_net(state_inputs)[action_batch]\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        # target_net is the approximation of the perfect policy network\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # TODO: change max to min to select DRC setting that yields minimum DRC values\n",
    "            next_state_values[0] = target_net(next_state_inputs).min(0).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "        # Compute Huber loss\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq8clj_DEupn"
   },
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and obtain the initial ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop.\n",
    "\n",
    "Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50\n",
    "episodes are scheduled so training does not take too long. However, 50\n",
    "episodes is insufficient for to observe good performance on CartPole.\n",
    "You should see the model constantly achieve 500 steps within 600 training\n",
    "episodes. Training RL agents can be a noisy process, so restarting training\n",
    "can produce better results if convergence is not observed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jG4TnOK4Eupn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 150\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = observation\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKNVv055Eupn"
   },
   "source": [
    "Here is the diagram that illustrates the overall resulting data flow.\n",
    "\n",
    ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
    "\n",
    "Actions are chosen either randomly or based on a policy, getting the next\n",
    "step sample from the gym environment. We record the results in the\n",
    "replay memory and also run optimization step on every iteration.\n",
    "Optimization picks a random batch from the replay memory to do training of the\n",
    "new policy. The \"older\" target_net is also used in optimization to compute the\n",
    "expected Q values. A soft update of its weights are performed at every step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create API for saving and loading memory data\n",
    "def saveMemoryData(memory, path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMemoryData(memory, path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Test pipeline\n",
    "\n",
    "Use trained model to help select optimal actions until finishing 1 trial, and compare a random action selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create actions for network input\n",
    "df_actions = env.actions.copy()\n",
    "df_actions.loc[:, 'drc'] = np.ones(len(df_actions)) * -1\n",
    "actions = df_actions.to_numpy()\n",
    "\n",
    "def select_test_action(state, model):\n",
    "    global actions\n",
    "    # select action according to policy\n",
    "    with torch.no_grad():\n",
    "        inputs = np.vstack((state, actions))\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        # select the action with minimum DRC output as optimum action\n",
    "        action = model(inputs).min(0).indices.view(1, 1)\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 action 9 DRC = 393467\n",
      "\n",
      "iteration 1 action 9 DRC = 72664\n",
      "\n",
      "iteration 2 action 9 DRC = 23495\n",
      "\n",
      "iteration 3 action 9 DRC = 15014\n",
      "\n",
      "iteration 4 action 9 DRC = 1719\n",
      "\n",
      "iteration 5 action 9 DRC = 1260\n",
      "\n",
      "iteration 6 action 9 DRC = 1146\n",
      "\n",
      "iteration 7 action 9 DRC = 458\n",
      "\n",
      "iteration 8 action 9 DRC = 229\n",
      "\n",
      "iteration 9 action 9 DRC = 114\n",
      "\n",
      "iteration 10 action 9 DRC = 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15530/1370369620.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if int(self._curr_drc * self.maxdrc) == 0:\n",
      "/tmp/ipykernel_15530/1370369620.py:136: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  drc = np.array(self.np_random.integers(0, int(self._curr_drc * self.maxdrc), size=1, dtype=int) / self.maxdrc)\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "count = 0\n",
    "# start trial\n",
    "env.reset()\n",
    "action = select_test_action(observation, policy_net)\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "while not(terminated or truncated):\n",
    "    print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "    # start next iteration\n",
    "    count += 1\n",
    "    action = select_test_action(observation, policy_net)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Save model. Load model and run test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to torch script via tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\n'numpy.int64' object in attribute 'Linear.in_features' is not a valid constant.\nValid constants are:\n1. a nn.ModuleList\n2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}\n3. a list or tuple of (2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# converting to Torch Script via annotation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scriptModule \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_script.py:1324\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1323\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:556\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nn_module, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mRecursiveScriptModule)\n\u001b[1;32m    555\u001b[0m check_module_initialized(nn_module)\n\u001b[0;32m--> 556\u001b[0m concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mget_module_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    558\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:505\u001b[0m, in \u001b[0;36mget_module_concrete_type\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn_module\u001b[38;5;241m.\u001b[39m_concrete_type\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m share_types:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# Look into the store of cached JIT types\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mconcrete_type_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Get a concrete type directly, without trying to re-use an existing JIT\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# type from the type store.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m infer_concrete_type_builder(nn_module, share_types)\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:438\u001b[0m, in \u001b[0;36mConcreteTypeStore.get_or_create_concrete_type\u001b[0;34m(self, nn_module)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_concrete_type\u001b[39m(\u001b[38;5;28mself\u001b[39m, nn_module):\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Infer a ConcreteType from this `nn.Module` instance. Underlying JIT\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    types are re-used if possible.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m \u001b[43minfer_concrete_type_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     nn_module_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(nn_module)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nn_module_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_store:\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:284\u001b[0m, in \u001b[0;36minfer_concrete_type_builder\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    279\u001b[0m     sub_concrete_type \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mConcreteModuleType\u001b[38;5;241m.\u001b[39mfrom_jit_type(\n\u001b[1;32m    280\u001b[0m         attr_type\u001b[38;5;241m.\u001b[39mtype()\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# otherwise we get the concrete module type for item and add it to concrete_type\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     sub_concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mget_module_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m concrete_type_builder\u001b[38;5;241m.\u001b[39madd_module(name, sub_concrete_type)\n\u001b[1;32m    287\u001b[0m added_names\u001b[38;5;241m.\u001b[39madd(name)\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:505\u001b[0m, in \u001b[0;36mget_module_concrete_type\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn_module\u001b[38;5;241m.\u001b[39m_concrete_type\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m share_types:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# Look into the store of cached JIT types\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mconcrete_type_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Get a concrete type directly, without trying to re-use an existing JIT\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# type from the type store.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m infer_concrete_type_builder(nn_module, share_types)\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:438\u001b[0m, in \u001b[0;36mConcreteTypeStore.get_or_create_concrete_type\u001b[0;34m(self, nn_module)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_concrete_type\u001b[39m(\u001b[38;5;28mself\u001b[39m, nn_module):\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Infer a ConcreteType from this `nn.Module` instance. Underlying JIT\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    types are re-used if possible.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m \u001b[43minfer_concrete_type_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     nn_module_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(nn_module)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nn_module_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_store:\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:328\u001b[0m, in \u001b[0;36minfer_concrete_type_builder\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(nn_module, name)\n\u001b[1;32m    327\u001b[0m     concrete_type_builder\u001b[38;5;241m.\u001b[39madd_constant(\n\u001b[0;32m--> 328\u001b[0m         name, \u001b[43m_get_valid_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    330\u001b[0m     added_names\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# populate overloads\u001b[39;00m\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_recursive.py:142\u001b[0m, in \u001b[0;36m_get_valid_constant\u001b[0;34m(attr, v, owner_type)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_get_valid_constant(attr, x, owner_type) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m v)\n\u001b[1;32m    141\u001b[0m constants \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(torch\u001b[38;5;241m.\u001b[39mtypename(typ) \u001b[38;5;28;01mfor\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m _constant_types)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m     textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(\u001b[38;5;28mtype\u001b[39m(v))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object in attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mowner_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid constant.\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124m    Valid constants are:\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124m    1. a nn.ModuleList\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124m    2. a value of type \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124m    3. a list or tuple of (2)\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: \n'numpy.int64' object in attribute 'Linear.in_features' is not a valid constant.\nValid constants are:\n1. a nn.ModuleList\n2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}\n3. a list or tuple of (2)\n"
     ]
    }
   ],
   "source": [
    "# converting to Torch Script via annotation\n",
    "scriptModule = torch.jit.script(policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scriptModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# serialize model to a file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m modelName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/DQN_FC.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mscriptModule\u001b[49m\u001b[38;5;241m.\u001b[39msave(modelName)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scriptModule' is not defined"
     ]
    }
   ],
   "source": [
    "# serialize model to a file\n",
    "modelName = \"./model/DQN_FC.pt\"\n",
    "scriptModule.save(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify C++ model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The provided filename ./model/DQN_FC.pt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loadModelName \u001b[38;5;241m=\u001b[39m modelName\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloadModelName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Data/Research/DLPnR/DL-code-for-detailed-routing/env/lib/python3.10/site-packages/torch/jit/_serialization.py:152\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(f):  \u001b[38;5;66;03m# type: ignore[type-var]\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[str-bytes-safe]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(f):\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[str-bytes-safe]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The provided filename ./model/DQN_FC.pt does not exist"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "loadModelName = modelName\n",
    "model = torch.jit.load(loadModelName)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# start trial\u001b[39;00m\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 5\u001b[0m action \u001b[38;5;241m=\u001b[39m select_test_action(observation, \u001b[43mmodel\u001b[49m)\n\u001b[1;32m      6\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(terminated \u001b[38;5;129;01mor\u001b[39;00m truncated):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "count = 0\n",
    "# start trial\n",
    "env.reset()\n",
    "action = select_test_action(observation, model)\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "while not(terminated or truncated):\n",
    "    print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "    # start next iteration\n",
    "    count += 1\n",
    "    action = select_test_action(observation, model)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "print(f\"iteration {count} action {action} DRC = {int(observation[-1, -1] * 1e6)}\\n\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
